{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddb07b3",
   "metadata": {},
   "source": [
    "# TimesNet Mid-Heavy Configuration - Financial Data Training\n",
    "\n",
    "This notebook contains a **mid-heavy TimesNet configuration** optimized for:\n",
    "- High-performance training with substantial computational resources\n",
    "- Complex pattern recognition in financial time series\n",
    "- Production-ready model development\n",
    "- Advanced feature extraction and long-range dependencies\n",
    "\n",
    "**Dataset**: Financial time series with 4 targets + 114 covariates (118 total features)\n",
    "**Training Time**: ~20-40 minutes per epoch\n",
    "**Memory Requirements**: High (recommend 8GB+ GPU memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cada87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "from models.TimesNet import Model as TimesNet\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate\n",
    "from utils.metrics import metric\n",
    "from utils.logger import logger\n",
    "from data_provider.data_loader import Dataset_Custom\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üíª Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf3b938",
   "metadata": {},
   "source": [
    "## üîß Mid-Heavy Configuration Parameters\n",
    "\n",
    "**Purpose**: High-capacity training for complex pattern recognition and production deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea22c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# MID-HEAVY CONFIGURATION - TIMESNET\n",
    "# ================================\n",
    "\n",
    "class MidHeavyConfig:\n",
    "    # === DATA CONFIGURATION ===\n",
    "    data = 'custom'                    # Dataset type (custom for prepared financial data)\n",
    "    root_path = './data/'              # Root directory for data files\n",
    "    data_path = 'prepared_financial_data.csv'  # Main data file\n",
    "    features = 'M'                     # Forecasting mode: 'M'=Multivariate, 'S'=Univariate, 'MS'=Multivariate-to-Univariate\n",
    "    target = 'log_Close'               # Primary target column (for 'S' mode)\n",
    "    freq = 'b'                         # Time frequency: 'b'=business day, 'h'=hourly, 'd'=daily\n",
    "    \n",
    "    # === SEQUENCE PARAMETERS ===\n",
    "    seq_len = 200                      # Input sequence length (lookback window) - MID-HEAVY: longer context\n",
    "    label_len = 20                     # Start token length for decoder input (overlap with seq_len)\n",
    "    pred_len = 20                      # Prediction horizon (how many steps to forecast) - MID-HEAVY: longer predictions\n",
    "    \n",
    "    # === TRAIN/VAL/TEST SPLITS ===\n",
    "    val_len = 20                       # Validation set length in time steps\n",
    "    test_len = 20                      # Test set length in time steps\n",
    "    prod_len = 20                      # Production forecast length (future predictions beyond data)\n",
    "    \n",
    "    # === TIMESNET MODEL ARCHITECTURE ===\n",
    "    # Core dimensions\n",
    "    enc_in = 118                       # Encoder input size (total features: 4 targets + 114 covariates)\n",
    "    dec_in = 118                       # Decoder input size (usually same as enc_in)\n",
    "    c_out = 118                        # Output size (must match enc_in to avoid dimension mismatch)\n",
    "    d_model = 128                      # Model dimension (embedding size) - MID-HEAVY: large capacity\n",
    "    d_ff = 256                         # Feed-forward network dimension - MID-HEAVY: large FFN\n",
    "    \n",
    "    # Attention mechanism\n",
    "    n_heads = 8                        # Number of attention heads - MID-HEAVY: more heads for complex patterns\n",
    "    e_layers = 4                       # Number of encoder layers - MID-HEAVY: deeper network\n",
    "    d_layers = 2                       # Number of decoder layers - MID-HEAVY: deeper decoder\n",
    "    \n",
    "    # TimesNet specific parameters\n",
    "    top_k = 8                          # Top-k frequencies for TimesNet decomposition - MID-HEAVY: more frequencies\n",
    "    num_kernels = 8                    # Number of convolution kernels in Inception blocks - MID-HEAVY: more kernels\n",
    "    \n",
    "    # Regularization\n",
    "    dropout = 0.15                     # Dropout rate for regularization - MID-HEAVY: higher dropout for large model\n",
    "    \n",
    "    # Additional model settings\n",
    "    embed = 'timeF'                    # Time feature embedding: 'timeF'=time features, 'fixed'=learnable, 'learned'=learned\n",
    "    activation = 'gelu'                # Activation function: 'gelu', 'relu', 'swish'\n",
    "    factor = 1                         # Attention factor (usually 1)\n",
    "    distil = True                      # Whether to use knowledge distillation\n",
    "    moving_avg = 50                    # Moving average window for trend decomposition - MID-HEAVY: longer window\n",
    "    output_attention = False           # Whether to output attention weights\n",
    "    \n",
    "    # === TRAINING CONFIGURATION ===\n",
    "    train_epochs = 50                  # Number of training epochs - MID-HEAVY: more epochs for convergence\n",
    "    batch_size = 16                    # Batch size - MID-HEAVY: smaller batch due to larger model\n",
    "    learning_rate = 0.0005             # Learning rate - MID-HEAVY: smaller for stable training\n",
    "    patience = 15                      # Early stopping patience - MID-HEAVY: more patience for complex model\n",
    "    lradj = 'type1'                    # Learning rate adjustment strategy\n",
    "    \n",
    "    # Loss and optimization\n",
    "    loss = 'MSE'                       # Loss function: 'MSE', 'MAE', 'Huber'\n",
    "    use_amp = True                     # Automatic mixed precision (recommended for large models)\n",
    "    \n",
    "    # System settings\n",
    "    num_workers = 8                    # DataLoader workers - MID-HEAVY: more workers for data loading\n",
    "    seed = 2024                        # Random seed for reproducibility\n",
    "    \n",
    "    # Task specific\n",
    "    task_name = 'short_term_forecast'  # Task type: 'short_term_forecast' for financial prediction\n",
    "    \n",
    "    # Experiment tracking\n",
    "    des = 'mid_heavy_config'           # Experiment description\n",
    "    checkpoints = f'./checkpoints/TimesNet_mid_heavy_{datetime.now().strftime(\"%Y%m%d_%H%M\")}'\n",
    "    \n",
    "# Create config instance\n",
    "args = MidHeavyConfig()\n",
    "\n",
    "print(\"üîß Mid-Heavy Configuration Loaded:\")\n",
    "print(f\"   üìè Sequence Length: {args.seq_len}\")\n",
    "print(f\"   üéØ Prediction Length: {args.pred_len}\")\n",
    "print(f\"   üß† Model Dimension: {args.d_model}\")\n",
    "print(f\"   ‚ö° Epochs: {args.train_epochs}\")\n",
    "print(f\"   üìä Batch Size: {args.batch_size}\")\n",
    "print(f\"   üîÑ Attention Heads: {args.n_heads}\")\n",
    "print(f\"   üèóÔ∏è Encoder Layers: {args.e_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f456b2",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Tweakable Parameters\n",
    "\n",
    "Modify these parameters to experiment with different mid-heavy configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e149bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# TWEAKABLE PARAMETERS - EXPERIMENT\n",
    "# ================================\n",
    "\n",
    "# Modify these for mid-heavy experiments:\n",
    "\n",
    "# --- Sequence parameters (affect context and prediction complexity) ---\n",
    "args.seq_len = 200         # Try: 150, 200, 300 (longer = more historical context)\n",
    "args.pred_len = 20         # Try: 15, 20, 30 (longer = more challenging forecasting)\n",
    "args.label_len = 20        # Try: 15, 20, 30 (usually 10-20% of seq_len)\n",
    "\n",
    "# --- Model architecture (affect capacity and computational cost) ---\n",
    "args.d_model = 128         # Try: 96, 128, 192 (larger = more representation power)\n",
    "args.d_ff = 256            # Try: 192, 256, 384 (usually 1.5-2x d_model)\n",
    "args.n_heads = 8           # Try: 6, 8, 12 (must divide d_model evenly)\n",
    "args.e_layers = 4          # Try: 3, 4, 6 (more layers = deeper feature extraction)\n",
    "args.d_layers = 2          # Try: 1, 2, 3 (decoder depth)\n",
    "\n",
    "# --- TimesNet specific (affect frequency decomposition) ---\n",
    "args.top_k = 8             # Try: 6, 8, 10 (more frequencies = richer patterns)\n",
    "args.num_kernels = 8       # Try: 6, 8, 12 (more kernels = more feature maps)\n",
    "args.moving_avg = 50       # Try: 30, 50, 75 (trend decomposition window)\n",
    "\n",
    "# --- Training parameters (affect learning and convergence) ---\n",
    "args.train_epochs = 50     # Try: 30, 50, 80\n",
    "args.batch_size = 16       # Try: 8, 16, 24 (smaller for large models)\n",
    "args.learning_rate = 0.0005  # Try: 0.0003, 0.0005, 0.001\n",
    "args.dropout = 0.15        # Try: 0.1, 0.15, 0.2 (higher for larger models)\n",
    "args.patience = 15         # Try: 10, 15, 20\n",
    "\n",
    "# --- Loss functions to experiment with ---\n",
    "# args.loss = 'MSE'        # Standard mean squared error\n",
    "# args.loss = 'MAE'        # Mean absolute error (robust to outliers)\n",
    "# args.loss = 'Huber'      # Combination of MSE and MAE\n",
    "# args.loss = 'MAPE'       # Mean absolute percentage error\n",
    "# args.loss = 'SMAPE'      # Symmetric mean absolute percentage error\n",
    "\n",
    "print(\"üéõÔ∏è Parameters ready for tweaking\")\n",
    "print(\"üí° Tip: Start with default values, then adjust one parameter at a time\")\n",
    "print(\"‚ö†Ô∏è  Note: Larger models require more GPU memory and training time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e15fb",
   "metadata": {},
   "source": [
    "## üìä Data Loading and Preparation\n",
    "\n",
    "Load the prepared financial dataset with all targets and covariates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and random seeds\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "print(f\"üé≤ Random seed set to: {args.seed}\")\n",
    "print(f\"üíª Using device: {device}\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(args.checkpoints, exist_ok=True)\n",
    "print(f\"üìÅ Checkpoints will be saved to: {args.checkpoints}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea04c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the data\n",
    "data_path = os.path.join(args.root_path, args.data_path)\n",
    "print(f\"üìÇ Loading data from: {data_path}\")\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"‚ùå Data file not found: {data_path}\")\n",
    "    print(\"Please run the data preparation script first:\")\n",
    "    print(\"python example_data_preparation.py\")\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"‚úÖ Data loaded successfully\")\n",
    "    print(f\"   üìä Shape: {df.shape}\")\n",
    "    print(f\"   üìÖ Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"   üéØ Target columns: log_Open, log_High, log_Low, log_Close\")\n",
    "    print(f\"   üîß Feature columns: {df.shape[1] - 5} (excluding date and targets)\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(\"\\nüìã First 3 rows:\")\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca7ebe",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Model Setup and Data Loaders\n",
    "\n",
    "Create TimesNet model and prepare data loaders for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "print(\"üîÑ Creating data loaders...\")\n",
    "\n",
    "# Training data loader\n",
    "train_data = Dataset_Custom(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag='train',\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    target=args.target,\n",
    "    timeenc=0 if args.embed != 'timeF' else 1,\n",
    "    freq=args.freq,\n",
    "    val_len=args.val_len,\n",
    "    test_len=args.test_len\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Validation data loader\n",
    "val_data = Dataset_Custom(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag='val',\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    target=args.target,\n",
    "    timeenc=0 if args.embed != 'timeF' else 1,\n",
    "    freq=args.freq,\n",
    "    val_len=args.val_len,\n",
    "    test_len=args.test_len\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Test data loader\n",
    "test_data = Dataset_Custom(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag='test',\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    target=args.target,\n",
    "    timeenc=0 if args.embed != 'timeF' else 1,\n",
    "    freq=args.freq,\n",
    "    val_len=args.val_len,\n",
    "    test_len=args.test_len\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data loaders created:\")\n",
    "print(f\"   üéì Training batches: {len(train_loader)}\")\n",
    "print(f\"   üîç Validation batches: {len(val_loader)}\")\n",
    "print(f\"   üß™ Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TimesNet model\n",
    "print(\"üß† Initializing TimesNet model...\")\n",
    "\n",
    "model = TimesNet(args).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ TimesNet model initialized\")\n",
    "print(f\"   üî¢ Total parameters: {total_params:,}\")\n",
    "print(f\"   üéØ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   üíæ Model size: ~{total_params * 4 / 1e6:.1f} MB\")\n",
    "\n",
    "# Setup optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
    "\n",
    "print(f\"‚öôÔ∏è Optimizer: Adam (lr={args.learning_rate})\")\n",
    "print(f\"üìâ Loss function: {args.loss}\")\n",
    "print(f\"‚è∞ Early stopping patience: {args.patience}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a73cc2",
   "metadata": {},
   "source": [
    "## üöÄ Training Loop\n",
    "\n",
    "Train the TimesNet model with progress tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device, use_amp=False):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    if use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        \n",
    "        # Decoder input\n",
    "        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "        \n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                # Focus loss on target columns (first 4 columns)\n",
    "                loss = criterion(outputs[:, -args.pred_len:, :4], batch_y[:, -args.pred_len:, :4])\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            # Focus loss on target columns (first 4 columns)\n",
    "            loss = criterion(outputs[:, -args.pred_len:, :4], batch_y[:, -args.pred_len:, :4])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Progress update every 20% of batches\n",
    "        if (i + 1) % max(1, num_batches // 5) == 0:\n",
    "            print(f\"    Batch {i+1}/{num_batches} - Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in val_loader:\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "            \n",
    "            # Decoder input\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "            \n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            # Focus loss on target columns (first 4 columns)\n",
    "            loss = criterion(outputs[:, -args.pred_len:, :4], batch_y[:, -args.pred_len:, :4])\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "print(\"üéØ Training functions defined\")\n",
    "print(\"üìù Ready to start training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(f\"üöÄ Starting training for {args.train_epochs} epochs...\")\n",
    "print(f\"‚ö° Using AMP: {args.use_amp}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(args.train_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print(f\"\\nüìä Epoch {epoch+1}/{args.train_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device, args.use_amp)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate adjustment\n",
    "    adjust_learning_rate(optimizer, epoch + 1, args)\n",
    "    \n",
    "    # Record losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"    üìà Train Loss: {train_loss:.6f}\")\n",
    "    print(f\"    üìâ Val Loss: {val_loss:.6f}\")\n",
    "    print(f\"    ‚è±Ô∏è  Epoch Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping(val_loss, model, args.checkpoints)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"\\n‚èπÔ∏è  Early stopping triggered\")\n",
    "        break\n",
    "    \n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint_path = os.path.join(args.checkpoints, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"    üíæ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Training completed!\")\n",
    "print(f\"‚è∞ Total training time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "print(f\"üìä Final train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"üìâ Final val loss: {val_losses[-1]:.6f}\")\n",
    "print(f\"üèÜ Best val loss: {min(val_losses):.6f} (epoch {val_losses.index(min(val_losses))+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8334d609",
   "metadata": {},
   "source": [
    "## üìà Training Visualization\n",
    "\n",
    "Plot training and validation loss curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cbcfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
    "plt.plot(val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss curves (log scale)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
    "plt.plot(val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Training and Validation Loss (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Training summary:\")\n",
    "print(f\"   üéØ Epochs completed: {len(train_losses)}\")\n",
    "print(f\"   üìà Loss improvement: {(train_losses[0] - train_losses[-1])/train_losses[0]*100:.1f}%\")\n",
    "print(f\"   ‚ö†Ô∏è  Overfitting check: {'Yes' if val_losses[-1] > min(val_losses) * 1.1 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f7c46",
   "metadata": {},
   "source": [
    "## üß™ Model Testing\n",
    "\n",
    "Evaluate the trained model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02850b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = os.path.join(args.checkpoints, 'checkpoint.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    print(f\"‚úÖ Loaded best model from: {best_model_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Using current model weights (best model checkpoint not found)\")\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nüß™ Testing model...\")\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        \n",
    "        # Decoder input\n",
    "        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "        \n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        \n",
    "        # Focus on target columns (first 4)\n",
    "        pred = outputs[:, -args.pred_len:, :4]\n",
    "        true = batch_y[:, -args.pred_len:, :4]\n",
    "        \n",
    "        loss = criterion(pred, true)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Store predictions and targets\n",
    "        all_predictions.append(pred.cpu().numpy())\n",
    "        all_targets.append(true.cpu().numpy())\n",
    "        \n",
    "        if i == 0:  # Show progress for first few batches\n",
    "            print(f\"    Test batch {i+1}/{len(test_loader)} - Loss: {loss.item():.6f}\")\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "# Concatenate all predictions and targets\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "print(f\"\\nüìä Test Results:\")\n",
    "print(f\"   üìâ Test Loss: {test_loss:.6f}\")\n",
    "print(f\"   üìê Predictions shape: {all_predictions.shape}\")\n",
    "print(f\"   üéØ Targets shape: {all_targets.shape}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"\\nüìà Additional Metrics:\")\n",
    "print(f\"   MAE: {mae:.6f}\")\n",
    "print(f\"   MSE: {mse:.6f}\")\n",
    "print(f\"   RMSE: {rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c82bf",
   "metadata": {},
   "source": [
    "## üîÆ Model Analysis\n",
    "\n",
    "Analyze model performance and visualize predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "target_names = ['log_Open', 'log_High', 'log_Low', 'log_Close']\n",
    "n_samples = min(3, all_predictions.shape[0])\n",
    "\n",
    "fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4*n_samples))\n",
    "if n_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for sample in range(n_samples):\n",
    "    for target in range(4):\n",
    "        ax = axes[sample, target]\n",
    "        \n",
    "        # Plot predictions vs targets\n",
    "        time_steps = range(args.pred_len)\n",
    "        ax.plot(time_steps, all_targets[sample, :, target], 'b-', label='True', alpha=0.7, linewidth=2)\n",
    "        ax.plot(time_steps, all_predictions[sample, :, target], 'r--', label='Predicted', alpha=0.7, linewidth=2)\n",
    "        \n",
    "        ax.set_title(f'Sample {sample+1}: {target_names[target]}')\n",
    "        ax.set_xlabel('Time Step')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Visualized {n_samples} prediction samples for all targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb69d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis by target\n",
    "print(\"\\nüéØ Performance by Target:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, target_name in enumerate(target_names):\n",
    "    target_pred = all_predictions[:, :, i]\n",
    "    target_true = all_targets[:, :, i]\n",
    "    \n",
    "    mae = np.mean(np.abs(target_pred - target_true))\n",
    "    mse = np.mean((target_pred - target_true) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Correlation\n",
    "    corr = np.corrcoef(target_pred.flatten(), target_true.flatten())[0, 1]\n",
    "    \n",
    "    print(f\"{target_name:12} | MAE: {mae:.6f} | RMSE: {rmse:.6f} | Corr: {corr:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Tips for improvement:\")\n",
    "print(\"   - If correlation is low: try longer seq_len or more layers\")\n",
    "print(\"   - If MAE is high: try different loss functions (MAE, Huber)\")\n",
    "print(\"   - If overfitting: increase dropout or reduce model size\")\n",
    "print(\"   - If underfitting: increase model capacity or training epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a55a4d",
   "metadata": {},
   "source": [
    "## üíæ Save Model and Results\n",
    "\n",
    "Save the trained model and experiment results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd225dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = os.path.join(args.checkpoints, 'final_model.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': vars(args),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'test_loss': test_loss,\n",
    "    'test_metrics': {\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse\n",
    "    }\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"üíæ Final model saved to: {final_model_path}\")\n",
    "\n",
    "# Save experiment summary\n",
    "summary_path = os.path.join(args.checkpoints, 'experiment_summary.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"TimesNet Mid-Heavy Configuration - Experiment Summary\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Configuration: {args.des}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Model Architecture:\\n\")\n",
    "    f.write(f\"  - Sequence Length: {args.seq_len}\\n\")\n",
    "    f.write(f\"  - Prediction Length: {args.pred_len}\\n\")\n",
    "    f.write(f\"  - Model Dimension: {args.d_model}\\n\")\n",
    "    f.write(f\"  - Feed-forward Dim: {args.d_ff}\\n\")\n",
    "    f.write(f\"  - Attention Heads: {args.n_heads}\\n\")\n",
    "    f.write(f\"  - Encoder Layers: {args.e_layers}\\n\")\n",
    "    f.write(f\"  - Decoder Layers: {args.d_layers}\\n\")\n",
    "    f.write(f\"  - Top-k Frequencies: {args.top_k}\\n\")\n",
    "    f.write(f\"  - Kernels: {args.num_kernels}\\n\")\n",
    "    f.write(f\"  - Total Parameters: {total_params:,}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Training Configuration:\\n\")\n",
    "    f.write(f\"  - Epochs: {len(train_losses)}\\n\")\n",
    "    f.write(f\"  - Batch Size: {args.batch_size}\\n\")\n",
    "    f.write(f\"  - Learning Rate: {args.learning_rate}\\n\")\n",
    "    f.write(f\"  - Dropout: {args.dropout}\\n\")\n",
    "    f.write(f\"  - Use AMP: {args.use_amp}\\n\")\n",
    "    f.write(f\"  - Training Time: {total_time:.1f}s\\n\\n\")\n",
    "    \n",
    "    f.write(\"Results:\\n\")\n",
    "    f.write(f\"  - Final Train Loss: {train_losses[-1]:.6f}\\n\")\n",
    "    f.write(f\"  - Final Val Loss: {val_losses[-1]:.6f}\\n\")\n",
    "    f.write(f\"  - Best Val Loss: {min(val_losses):.6f}\\n\")\n",
    "    f.write(f\"  - Test Loss: {test_loss:.6f}\\n\")\n",
    "    f.write(f\"  - Test MAE: {mae:.6f}\\n\")\n",
    "    f.write(f\"  - Test RMSE: {rmse:.6f}\\n\")\n",
    "\n",
    "print(f\"üìù Experiment summary saved to: {summary_path}\")\n",
    "\n",
    "print(\"\\nüéâ Mid-Heavy Configuration Training Complete!\")\n",
    "print(f\"üìÅ All results saved in: {args.checkpoints}\")\n",
    "print(\"\\nüí° Next steps:\")\n",
    "print(\"   - Compare with light and medium configurations\")\n",
    "print(\"   - Try different hyperparameters\")\n",
    "print(\"   - Experiment with different loss functions\")\n",
    "print(\"   - Implement production forecasting\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
