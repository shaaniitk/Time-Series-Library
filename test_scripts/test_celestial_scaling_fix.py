#!/usr/bin/env python3
"""
Test script to validate celestial training scaling fixes
Tests that scaling is applied consistently in loss computation
"""

import os
import sys
import re
from pathlib import Path

def test_celestial_scaling_fixes():
    """Test that scaling fixes are properly implemented in celestial training script"""
    print("ğŸ§ª Testing Celestial Training Scaling Fixes")
    print("=" * 50)
    
    # Read the celestial training script
    try:
        with open('scripts/train/train_celestial_direct.py', 'r') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ Error reading celestial training script: {e}")
        return False
    
    # Test 1: Check for scaling in training loss computation
    print("\n1. Testing Training Loss Scaling Fix...")
    
    # Look for the scaling fix pattern in training
    training_scaling_pattern = r'train_data\.scaler.*transform.*gt_slice'
    if re.search(training_scaling_pattern, content, re.DOTALL):
        print("   âœ… Training loss scaling fix found")
    else:
        print("   âŒ Training loss scaling fix not found")
        return False
    
    # Test 2: Check for scaling in validation loss computation
    print("\n2. Testing Validation Loss Scaling Fix...")
    
    validation_scaling_pattern = r'vali_data\.scaler.*transform.*gt_slice'
    if re.search(validation_scaling_pattern, content, re.DOTALL):
        print("   âœ… Validation loss scaling fix found")
    else:
        print("   âŒ Validation loss scaling fix not found")
        return False
    
    # Test 3: Check for error handling
    print("\n3. Testing Error Handling...")
    
    error_handling_pattern = r'except.*Exception.*scaling failed'
    if re.search(error_handling_pattern, content, re.DOTALL):
        print("   âœ… Error handling for scaling found")
    else:
        print("   âŒ Error handling for scaling not found")
        return False
    
    # Test 4: Check for metrics scaling (optional)
    print("\n4. Testing Metrics Scaling...")
    
    metrics_scaling_pattern = r'test_data\.scaler.*transform.*true_scaled'
    if re.search(metrics_scaling_pattern, content, re.DOTALL):
        print("   âœ… Metrics scaling found")
    else:
        print("   âš ï¸  Metrics scaling not found (optional)")
    
    # Test 5: Check that old unscaled loss computation is replaced
    print("\n5. Testing Old Code Replacement...")
    
    # Look for the old problematic pattern
    old_pattern = r'loss = criterion\(out_slice, gt_slice\)(?!\s*#)'
    old_matches = re.findall(old_pattern, content)
    
    if len(old_matches) == 0:
        print("   âœ… Old unscaled loss computation properly replaced")
    else:
        print(f"   âš ï¸  Found {len(old_matches)} instances of old unscaled loss computation")
        print("   This might be in fallback error handling, which is acceptable")
    
    return True

def test_scaling_consistency():
    """Test that scaling is consistent across all phases"""
    print("\nğŸ§ª Testing Scaling Consistency")
    print("=" * 50)
    
    try:
        with open('scripts/train/train_celestial_direct.py', 'r') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ Error reading file: {e}")
        return False
    
    # Count scaling operations
    train_scaling_count = len(re.findall(r'train_data\.scaler.*transform', content))
    vali_scaling_count = len(re.findall(r'vali_data\.scaler.*transform', content))
    test_scaling_count = len(re.findall(r'test_data\.scaler.*transform', content))
    
    print(f"   ğŸ“Š Training scaling operations: {train_scaling_count}")
    print(f"   ğŸ“Š Validation scaling operations: {vali_scaling_count}")
    print(f"   ğŸ“Š Test scaling operations: {test_scaling_count}")
    
    if train_scaling_count >= 1 and vali_scaling_count >= 1:
        print("   âœ… Scaling applied in both training and validation")
        return True
    else:
        print("   âŒ Scaling not consistently applied")
        return False

def main():
    """Run all scaling tests"""
    print("ğŸš€ Celestial Training Scaling Fixes Validation")
    print("=" * 60)
    
    tests = [
        ("Scaling Fixes Implementation", test_celestial_scaling_fixes),
        ("Scaling Consistency", test_scaling_consistency),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ Test {test_name} crashed: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“Š SCALING FIXES VALIDATION SUMMARY")
    print("=" * 60)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… PASSED" if result else "âŒ FAILED"
        print(f"{test_name:<40} {status}")
        if result:
            passed += 1
    
    print("-" * 60)
    print(f"Overall: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nğŸ‰ All scaling fixes validated successfully!")
        print("\nğŸ“‹ FIXES APPLIED:")
        print("1. âœ… Training loss now uses scaled targets")
        print("2. âœ… Validation loss now uses scaled targets") 
        print("3. âœ… Error handling for scaling failures")
        print("4. âœ… Optional metrics scaling for consistency")
        print("5. âœ… Follows same pattern as standard framework")
        print("\nğŸš€ Celestial training script is now scaling-consistent!")
        return 0
    else:
        print("\nâš ï¸  Some scaling fixes need attention")
        return 1

if __name__ == "__main__":
    exit(main())