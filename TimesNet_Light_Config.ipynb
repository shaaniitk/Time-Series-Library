{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f277d14",
   "metadata": {},
   "source": [
    "## ‚ö° ULTRA-FAST Quick Test Configuration\n",
    "\n",
    "**üöÄ For immediate testing and debugging - runs in under 2 minutes!**\n",
    "\n",
    "This ultra-fast configuration is designed for:\n",
    "- **Immediate feedback** during development\n",
    "- **Quick sanity checks** that everything works\n",
    "- **Fast iterations** while debugging\n",
    "- **Proof of concept** validation\n",
    "\n",
    "**Expected time: 30 seconds - 2 minutes total**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa870c3",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è **Important Note**\n",
    "\n",
    "This notebook uses the **standard TSLib data loading approach** with `Dataset_Custom` for maximum compatibility and performance. The prepared financial data (`prepared_financial_data.csv`) contains all targets and covariates in a single file that's ready for TimesNet training.\n",
    "\n",
    "**Data Structure:**\n",
    "- ‚úÖ **Targets**: 4 columns (log_Open, log_High, log_Low, log_Close)\n",
    "- ‚úÖ **Covariates**: 114 columns (87 dynamic + 26 static + 1 time_delta)\n",
    "- ‚úÖ **Total**: 118 features aligned on business days\n",
    "- ‚úÖ **Ready to use**: No additional data preparation needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7fe4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° ULTRA-FAST Configuration Loaded:\n",
      "   üìè Sequence: 20 ‚Üí 3\n",
      "   üß† Model: d_model=16, layers=1\n",
      "   ‚ö° Epochs: 3 (should complete in ~2 minutes)\n",
      "   üìä Batch Size: 64\n",
      "   üöÄ Expected Total Time: 30 seconds - 2 minutes\n",
      "\n",
      "üí° This config prioritizes SPEED over accuracy for quick testing!\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° ULTRA-FAST CONFIGURATION - FOR IMMEDIATE TESTING\n",
    "# This will run in under 2 minutes!\n",
    "from datetime import datetime\n",
    "class UltraFastConfig:\n",
    "    \"\"\"Ultra-fast configuration for immediate testing and debugging\"\"\"\n",
    "    \n",
    "    # === DATA CONFIGURATION ===\n",
    "    data = 'custom'\n",
    "    root_path = './data/'\n",
    "    data_path = 'prepared_financial_data.csv'\n",
    "    features = 'M'\n",
    "    target = 'log_Close'\n",
    "    freq = 'b'\n",
    "    \n",
    "    # === SEQUENCE PARAMETERS - MINIMAL ===\n",
    "    seq_len = 20                       # ULTRA-FAST: Very short sequences\n",
    "    label_len = 5                      # ULTRA-FAST: Minimal label length\n",
    "    pred_len = 3                       # ULTRA-FAST: Very short predictions\n",
    "    \n",
    "    # === SPLITS - TINY FOR SPEED ===\n",
    "    val_len = 5                        # ULTRA-FAST: Minimal validation\n",
    "    test_len = 5                       # ULTRA-FAST: Minimal test\n",
    "    prod_len = 3                       # ULTRA-FAST: Minimal production\n",
    "    \n",
    "    # === MODEL ARCHITECTURE - MINIMAL ===\n",
    "    enc_in = 118                       # Keep same (data structure requirement)\n",
    "    dec_in = 118                       \n",
    "    c_out = 118                        \n",
    "    d_model = 16                       # ULTRA-FAST: Tiny model dimension\n",
    "    d_ff = 32                          # ULTRA-FAST: Tiny feed-forward\n",
    "    \n",
    "    # === ATTENTION - MINIMAL ===\n",
    "    n_heads = 2                        # ULTRA-FAST: Minimal heads (must divide d_model)\n",
    "    e_layers = 1                       # ULTRA-FAST: Single encoder layer\n",
    "    d_layers = 1                       # ULTRA-FAST: Single decoder layer\n",
    "    \n",
    "    # === TIMESNET - MINIMAL ===\n",
    "    top_k = 2                          # ULTRA-FAST: Minimal frequencies\n",
    "    num_kernels = 2                    # ULTRA-FAST: Minimal kernels\n",
    "    \n",
    "    # === REGULARIZATION ===\n",
    "    dropout = 0.0                      # ULTRA-FAST: No dropout for speed\n",
    "    \n",
    "    # === ADDITIONAL SETTINGS ===\n",
    "    embed = 'timeF'\n",
    "    activation = 'gelu'\n",
    "    factor = 1\n",
    "    distil = False                     # ULTRA-FAST: No distillation\n",
    "    moving_avg = 5                     # ULTRA-FAST: Minimal moving average\n",
    "    output_attention = False\n",
    "    \n",
    "    # === TRAINING - ULTRA MINIMAL ===\n",
    "    train_epochs = 3                   # ULTRA-FAST: Just 3 epochs!\n",
    "    batch_size = 64                    # ULTRA-FAST: Larger batches for speed\n",
    "    learning_rate = 0.01               # ULTRA-FAST: Higher LR for faster convergence\n",
    "    patience = 2                       # ULTRA-FAST: Very low patience\n",
    "    lradj = 'type1'\n",
    "    \n",
    "    # === OPTIMIZATION ===\n",
    "    loss = 'MSE'\n",
    "    use_amp = True                     # ULTRA-FAST: Use AMP for speed\n",
    "    \n",
    "    # === SYSTEM ===\n",
    "    num_workers = 2                    # ULTRA-FAST: Minimal workers\n",
    "    seed = 2024\n",
    "    task_name = 'short_term_forecast'\n",
    "    des = 'ultra_fast_test'\n",
    "    checkpoints = f'./checkpoints/TimesNet_ultra_fast_{datetime.now().strftime(\"%Y%m%d_%H%M\")}'\n",
    "\n",
    "# Create ultra-fast config\n",
    "ultra_args = UltraFastConfig()\n",
    "\n",
    "print(\"‚ö° ULTRA-FAST Configuration Loaded:\")\n",
    "print(f\"   üìè Sequence: {ultra_args.seq_len} ‚Üí {ultra_args.pred_len}\")\n",
    "print(f\"   üß† Model: d_model={ultra_args.d_model}, layers={ultra_args.e_layers}\")\n",
    "print(f\"   ‚ö° Epochs: {ultra_args.train_epochs} (should complete in ~2 minutes)\")\n",
    "print(f\"   üìä Batch Size: {ultra_args.batch_size}\")\n",
    "print(f\"   üöÄ Expected Total Time: 30 seconds - 2 minutes\")\n",
    "print()\n",
    "print(\"üí° This config prioritizes SPEED over accuracy for quick testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f8c9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Configuration Options:\n",
      "1. ‚ö° ULTRA-FAST: Run switch_to_ultra_fast() - completes in ~2 minutes\n",
      "2. üí° LIGHT: Run switch_to_light() - completes in ~5-10 minutes\n",
      "\n",
      "üí° Recommendation: Start with ULTRA-FAST to verify everything works!\n",
      "\n",
      "‚ö° Switched to ULTRA-FAST configuration!\n",
      "üïê Expected completion time: 30 seconds - 2 minutes\n"
     ]
    }
   ],
   "source": [
    "# üéõÔ∏è CONFIGURATION SWITCHER - Choose your speed!\n",
    "\n",
    "def switch_to_ultra_fast():\n",
    "    \"\"\"Switch to ultra-fast configuration for immediate testing\"\"\"\n",
    "    global args\n",
    "    args = ultra_args\n",
    "    print(\"‚ö° Switched to ULTRA-FAST configuration!\")\n",
    "    print(\"üïê Expected completion time: 30 seconds - 2 minutes\")\n",
    "    return args\n",
    "\n",
    "def switch_to_light():\n",
    "    \"\"\"Switch to standard light configuration\"\"\"\n",
    "    global args\n",
    "    args = LightConfig()\n",
    "    print(\"üí° Switched to LIGHT configuration\")\n",
    "    print(\"üïê Expected completion time: 5-10 minutes\")\n",
    "    return args\n",
    "\n",
    "# üöÄ FOR IMMEDIATE TESTING - USE ULTRA-FAST!\n",
    "print(\"üéØ Configuration Options:\")\n",
    "print(\"1. ‚ö° ULTRA-FAST: Run switch_to_ultra_fast() - completes in ~2 minutes\")\n",
    "print(\"2. üí° LIGHT: Run switch_to_light() - completes in ~5-10 minutes\")\n",
    "print()\n",
    "print(\"üí° Recommendation: Start with ULTRA-FAST to verify everything works!\")\n",
    "print()\n",
    "\n",
    "# Automatically switch to ultra-fast for immediate testing\n",
    "args = switch_to_ultra_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab2459b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® NEW DEBUGGING MODE AVAILABLE:\n",
      "   Run switch_to_debugging() if ultra-fast is still too slow\n",
      "   This uses the absolute minimum possible configuration\n"
     ]
    }
   ],
   "source": [
    "# üö® DEBUGGING CONFIGURATION - For Troubleshooting Only\n",
    "# This is an EXTREMELY minimal config to test if basic functionality works\n",
    "\n",
    "class DebuggingConfig:\n",
    "    \"\"\"Ultra-minimal configuration for troubleshooting training bottlenecks\"\"\"\n",
    "    \n",
    "    # === DATA CONFIGURATION - TINY ===\n",
    "    seq_len = 12                       # MINIMAL: Only 12 input steps\n",
    "    label_len = 2                      # MINIMAL: Only 2 label steps\n",
    "    pred_len = 1                       # MINIMAL: Predict just 1 step\n",
    "    \n",
    "    # Data settings\n",
    "    data = 'prepared_financial_data'\n",
    "    root_path = './data/'\n",
    "    data_path = 'prepared_financial_data.csv'\n",
    "    features = 'M'                     # Multivariate\n",
    "    target = 'close'                   # Primary target\n",
    "    freq = 'd'                         # Daily frequency\n",
    "    \n",
    "    # === MODEL CONFIGURATION - TINY ===\n",
    "    enc_in = 118                       # Keep same (data requirement)\n",
    "    dec_in = 118                       \n",
    "    c_out = 118                        \n",
    "    d_model = 8                        # MINIMAL: Tiny dimension (smallest possible)\n",
    "    d_ff = 16                          # MINIMAL: Tiny feed-forward\n",
    "    \n",
    "    # === ATTENTION - MINIMAL ===\n",
    "    n_heads = 1                        # MINIMAL: Single head\n",
    "    e_layers = 1                       # MINIMAL: Single encoder layer\n",
    "    d_layers = 1                       # MINIMAL: Single decoder layer\n",
    "    \n",
    "    # === TIMESNET - MINIMAL ===\n",
    "    top_k = 1                          # MINIMAL: Single frequency\n",
    "    num_kernels = 1                    # MINIMAL: Single kernel\n",
    "    \n",
    "    # === REGULARIZATION ===\n",
    "    dropout = 0.0                      # No dropout\n",
    "    \n",
    "    # === ADDITIONAL SETTINGS ===\n",
    "    embed = 'timeF'\n",
    "    activation = 'gelu'\n",
    "    factor = 1\n",
    "    distil = False\n",
    "    moving_avg = 3                     # MINIMAL: Smallest moving average\n",
    "    output_attention = False\n",
    "    \n",
    "    # === TRAINING - ULTRA MINIMAL ===\n",
    "    train_epochs = 1                   # MINIMAL: Just 1 epoch!\n",
    "    batch_size = 16                    # MINIMAL: Small batch for debugging\n",
    "    learning_rate = 0.01               \n",
    "    patience = 1                       # MINIMAL: No patience\n",
    "    lradj = 'type1'\n",
    "    \n",
    "    # === OPTIMIZATION ===\n",
    "    loss = 'MSE'\n",
    "    use_amp = False                    # Disable AMP for debugging\n",
    "    \n",
    "    # === SYSTEM ===\n",
    "    num_workers = 0                    # MINIMAL: No multiprocessing\n",
    "    seed = 2024\n",
    "    task_name = 'short_term_forecast'\n",
    "    des = 'debug_test'\n",
    "    checkpoints = f'./checkpoints/TimesNet_debug_{datetime.now().strftime(\"%Y%m%d_%H%M\")}'\n",
    "\n",
    "# Create debugging config\n",
    "debug_args = DebuggingConfig()\n",
    "\n",
    "def switch_to_debugging():\n",
    "    \"\"\"Switch to debugging configuration for troubleshooting\"\"\"\n",
    "    global args\n",
    "    args = debug_args\n",
    "    print(\"üö® Switched to DEBUGGING configuration!\")\n",
    "    print(\"‚ö†Ô∏è This is EXTREMELY minimal - for troubleshooting only!\")\n",
    "    print(\"üïê Expected completion time: 10-30 seconds\")\n",
    "    print(f\"   üìè Sequence: {args.seq_len} ‚Üí {args.pred_len}\")\n",
    "    print(f\"   üß† Model: d_model={args.d_model}, single layer, single head\")\n",
    "    print(f\"   ‚ö° Epochs: {args.train_epochs} epoch\")\n",
    "    print(f\"   üìä Batch Size: {args.batch_size}\")\n",
    "    return args\n",
    "\n",
    "print(\"üö® NEW DEBUGGING MODE AVAILABLE:\")\n",
    "print(\"   Run switch_to_debugging() if ultra-fast is still too slow\")\n",
    "print(\"   This uses the absolute minimum possible configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13bb681b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.switch_to_debugging()>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_to_debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659c432",
   "metadata": {},
   "source": [
    "## üö® **URGENT: Read This If Training is Slow!**\n",
    "\n",
    "### üéØ **Quick Start for Immediate Results:**\n",
    "\n",
    "**If you're experiencing slow training, follow these steps RIGHT NOW:**\n",
    "\n",
    "**Step 1: üö® Try Emergency Mode**\n",
    "```python\n",
    "emergency_debug_mode()  # Should complete in <30 seconds\n",
    "```\n",
    "\n",
    "**Step 2: ‚ö° If that works, try Ultra-Fast**\n",
    "```python\n",
    "switch_to_ultra_fast()  # Should complete in <2 minutes\n",
    "```\n",
    "\n",
    "**Step 3: üìä If still slow, run diagnostics**\n",
    "```python\n",
    "# Run the quick speed test cell below\n",
    "# Then run the full diagnostics cell\n",
    "```\n",
    "\n",
    "### üîç **What Each Mode Does:**\n",
    "\n",
    "| Mode | Time | Purpose |\n",
    "|------|------|---------|\n",
    "| üö® **Emergency** | <30 sec | Test if ANYTHING works |\n",
    "| ‚ö° **Ultra-Fast** | <2 min | Quick experimentation |\n",
    "| üí° **Light** | <10 min | Normal light training |\n",
    "\n",
    "### ‚ö†Ô∏è **Common Issues & Solutions:**\n",
    "\n",
    "**Problem: First batch never completes**\n",
    "- ‚úÖ Run `emergency_debug_mode()`\n",
    "- ‚úÖ Check GPU availability\n",
    "- ‚úÖ Set `num_workers = 0`\n",
    "\n",
    "**Problem: Training very slow (>5 min)**\n",
    "- ‚úÖ Use `switch_to_debugging()`\n",
    "- ‚úÖ Reduce batch size to 8 or 16\n",
    "- ‚úÖ Disable AMP: `use_amp = False`\n",
    "\n",
    "**Problem: Out of memory**\n",
    "- ‚úÖ Reduce `batch_size` to 8\n",
    "- ‚úÖ Reduce `d_model` to 8\n",
    "- ‚úÖ Use `switch_to_debugging()`\n",
    "\n",
    "### üéØ **Success Criteria:**\n",
    "\n",
    "- ‚úÖ **Emergency mode**: Completes in 10-30 seconds\n",
    "- ‚úÖ **Ultra-fast mode**: Completes in 30 seconds - 2 minutes  \n",
    "- ‚úÖ **Light mode**: Completes in 2-10 minutes\n",
    "\n",
    "If emergency mode fails, there's a fundamental system issue (GPU, CUDA, data corruption, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5107c01",
   "metadata": {},
   "source": [
    "# TimesNet Light Configuration - Financial Data Training\n",
    "\n",
    "This notebook contains a **lightweight TimesNet configuration** optimized for:\n",
    "- Fast experimentation and testing\n",
    "- Quick iterations during development\n",
    "- Resource-constrained environments\n",
    "- Proof of concept validations\n",
    "\n",
    "**Dataset**: Financial time series with 4 targets + 114 covariates (118 total features)\n",
    "**Training Time**: ~5-10 minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29c498de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root found and added: d:\\workspace\\Time-Series-Library\n",
      "üìÅ Project root: d:\\workspace\\Time-Series-Library\n",
      "üêç Python path includes: ['d:\\\\workspace\\\\Time-Series-Library', 'C:\\\\Users\\\\mishr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python311.zip', 'C:\\\\Users\\\\mishr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\DLLs']\n",
      "‚úÖ All imports successful!\n",
      "‚úÖ All imports successful\n",
      "üî• PyTorch version: 2.7.1+cpu\n",
      "üíª Device: CPU\n",
      "‚ö†Ô∏è  No GPU detected - will use CPU (training will be slower)\n",
      "‚úÖ All imports successful!\n",
      "‚úÖ All imports successful\n",
      "üî• PyTorch version: 2.7.1+cpu\n",
      "üíª Device: CPU\n",
      "‚ö†Ô∏è  No GPU detected - will use CPU (training will be slower)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# === ROBUST PATH SETUP FOR GPU DEPLOYMENT ===\n",
    "# This handles both local development and GPU server deployment\n",
    "\n",
    "# Method 1: Auto-detect project root\n",
    "def setup_project_path():\n",
    "    \"\"\"Automatically detect and add the TimesNet project root to Python path\"\"\"\n",
    "    \n",
    "    # Try to find project root automatically\n",
    "    current_dir = os.getcwd()\n",
    "    possible_roots = [\n",
    "        current_dir,  # Current working directory\n",
    "        os.path.dirname(os.path.abspath('.')),  # Parent directory\n",
    "        os.path.dirname(os.path.abspath(__file__ if '__file__' in globals() else '.')),  # Script directory\n",
    "    ]\n",
    "    \n",
    "    # Check if this is a custom path (like Google Colab)\n",
    "    if '/content/drive/MyDrive' in current_dir or any('timesnet' in p.lower() for p in [current_dir]):\n",
    "        # Custom deployment path detected\n",
    "        print(f\"üîç Custom deployment detected: {current_dir}\")\n",
    "        if current_dir not in sys.path:\n",
    "            sys.path.insert(0, current_dir)\n",
    "            print(f\"‚úÖ Added to path: {current_dir}\")\n",
    "    \n",
    "    # Try each possible root\n",
    "    for root in possible_roots:\n",
    "        if root and os.path.exists(os.path.join(root, 'models')) and os.path.exists(os.path.join(root, 'utils')):\n",
    "            if root not in sys.path:\n",
    "                sys.path.insert(0, root)\n",
    "                print(f\"‚úÖ Project root found and added: {root}\")\n",
    "                return root\n",
    "    \n",
    "    # If not found, add current directory as fallback\n",
    "    if current_dir not in sys.path:\n",
    "        sys.path.insert(0, current_dir)\n",
    "        print(f\"‚ö†Ô∏è  Using current directory as fallback: {current_dir}\")\n",
    "    \n",
    "    return current_dir\n",
    "\n",
    "# Setup the project path\n",
    "project_root = setup_project_path()\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üêç Python path includes: {[p for p in sys.path[:3]]}\")\n",
    "\n",
    "# Try to import with error handling\n",
    "try:\n",
    "    from models.TimesNet import Model as TimesNet\n",
    "    from utils.tools import EarlyStopping, adjust_learning_rate\n",
    "    from utils.metrics import metric\n",
    "    from utils.logger import logger\n",
    "    from data_provider.data_loader import Dataset_Custom\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"\\nüîß TROUBLESHOOTING:\")\n",
    "    print(\"1. Verify you extracted the GPU package correctly\")\n",
    "    print(\"2. Check that these directories exist:\")\n",
    "    for dir_name in ['models', 'utils', 'data_provider', 'layers', 'exp']:\n",
    "        dir_path = os.path.join(project_root, dir_name)\n",
    "        exists = os.path.exists(dir_path)\n",
    "        print(f\"   {'‚úÖ' if exists else '‚ùå'} {dir_path}\")\n",
    "    \n",
    "    print(\"\\nüí° Quick fix - run this in the next cell:\")\n",
    "    print(\"import sys\")\n",
    "    print(f\"sys.path.insert(0, '{project_root}')\")\n",
    "    print(\"# Then re-run the imports\")\n",
    "    \n",
    "    raise\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üíª Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Enhanced GPU information\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    print(f\"‚ö° CUDA Version: {torch.version.cuda}\")\n",
    "    print(\"üéØ GPU acceleration will be used automatically!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected - will use CPU (training will be slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f443eab",
   "metadata": {},
   "source": [
    "## üîß Light Configuration Parameters\n",
    "\n",
    "**Purpose**: Fast training for quick experimentation and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f67920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Light Configuration Loaded:\n",
      "   üìè Sequence Length: 50\n",
      "   üéØ Prediction Length: 5\n",
      "   üß† Model Dimension: 32\n",
      "   ‚ö° Epochs: 10\n",
      "   üìä Batch Size: 32\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# LIGHT CONFIGURATION - TIMESNET\n",
    "# ================================\n",
    "\n",
    "class LightConfig:\n",
    "    # === DATA CONFIGURATION ===\n",
    "    data = 'custom'                    # Dataset type (custom for prepared financial data)\n",
    "    root_path = './data/'              # Root directory for data files\n",
    "    data_path = 'prepared_financial_data.csv'  # Main data file\n",
    "    features = 'M'                     # Forecasting mode: 'M'=Multivariate, 'S'=Univariate, 'MS'=Multivariate-to-Univariate\n",
    "    target = 'log_Close'               # Primary target column (for 'S' mode)\n",
    "    freq = 'b'                         # Time frequency: 'b'=business day, 'h'=hourly, 'd'=daily\n",
    "    \n",
    "    # === SEQUENCE PARAMETERS ===\n",
    "    seq_len = 50                       # Input sequence length (lookback window) - LIGHT: shorter for speed\n",
    "    label_len = 10                     # Start token length for decoder input (overlap with seq_len)\n",
    "    pred_len = 5                       # Prediction horizon (how many steps to forecast) - LIGHT: shorter predictions\n",
    "    \n",
    "    # === TRAIN/VAL/TEST SPLITS ===\n",
    "    val_len = 10                       # Validation set length in time steps\n",
    "    test_len = 10                      # Test set length in time steps\n",
    "    prod_len = 5                       # Production forecast length (future predictions beyond data)\n",
    "    \n",
    "    # === TIMESNET MODEL ARCHITECTURE ===\n",
    "    # Core dimensions\n",
    "    enc_in = 118                       # Encoder input size (total features: 4 targets + 114 covariates)\n",
    "    dec_in = 118                       # Decoder input size (usually same as enc_in)\n",
    "    c_out = 118                        # Output size (must match enc_in to avoid dimension mismatch)\n",
    "    d_model = 32                       # Model dimension (embedding size) - LIGHT: smaller for speed\n",
    "    d_ff = 64                          # Feed-forward network dimension - LIGHT: smaller FFN\n",
    "    \n",
    "    # Attention mechanism\n",
    "    n_heads = 4                        # Number of attention heads - LIGHT: fewer heads\n",
    "    e_layers = 2                       # Number of encoder layers - LIGHT: fewer layers\n",
    "    d_layers = 1                       # Number of decoder layers (usually 1 for forecasting)\n",
    "    \n",
    "    # TimesNet specific parameters\n",
    "    top_k = 3                          # Top-k frequencies for TimesNet decomposition - LIGHT: fewer frequencies\n",
    "    num_kernels = 3                    # Number of convolution kernels in Inception blocks - LIGHT: fewer kernels\n",
    "    \n",
    "    # Regularization\n",
    "    dropout = 0.1                      # Dropout rate for regularization\n",
    "    \n",
    "    # Additional model settings\n",
    "    embed = 'timeF'                    # Time feature embedding: 'timeF'=time features, 'fixed'=learnable, 'learned'=learned\n",
    "    activation = 'gelu'                # Activation function: 'gelu', 'relu', 'swish'\n",
    "    factor = 1                         # Attention factor (usually 1)\n",
    "    distil = True                      # Whether to use knowledge distillation\n",
    "    moving_avg = 25                    # Moving average window for trend decomposition\n",
    "    output_attention = False           # Whether to output attention weights\n",
    "    \n",
    "    # === TRAINING CONFIGURATION ===\n",
    "    train_epochs = 10                  # Number of training epochs - LIGHT: fewer epochs\n",
    "    batch_size = 32                    # Batch size - LIGHT: moderate batch size\n",
    "    learning_rate = 0.001              # Learning rate - LIGHT: slightly higher for faster convergence\n",
    "    patience = 5                       # Early stopping patience - LIGHT: less patience\n",
    "    lradj = 'type1'                    # Learning rate adjustment strategy\n",
    "    \n",
    "    # Loss and optimization\n",
    "    loss = 'MSE'                       # Loss function: 'MSE', 'MAE', 'Huber'\n",
    "    use_amp = False                    # Automatic mixed precision (can speed up training)\n",
    "    \n",
    "    # System settings\n",
    "    num_workers = 4                    # DataLoader workers - LIGHT: fewer workers\n",
    "    seed = 2024                        # Random seed for reproducibility\n",
    "    \n",
    "    # Task specific\n",
    "    task_name = 'short_term_forecast'  # Task type: 'short_term_forecast' for financial prediction\n",
    "    \n",
    "    # Experiment tracking\n",
    "    des = 'light_config'               # Experiment description\n",
    "    checkpoints = f'./checkpoints/TimesNet_light_{datetime.now().strftime(\"%Y%m%d_%H%M\")}'\n",
    "    \n",
    "# Create config instance\n",
    "args = LightConfig()\n",
    "\n",
    "print(\"üîß Light Configuration Loaded:\")\n",
    "print(f\"   üìè Sequence Length: {args.seq_len}\")\n",
    "print(f\"   üéØ Prediction Length: {args.pred_len}\")\n",
    "print(f\"   üß† Model Dimension: {args.d_model}\")\n",
    "print(f\"   ‚ö° Epochs: {args.train_epochs}\")\n",
    "print(f\"   üìä Batch Size: {args.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa0b97",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Tweakable Parameters\n",
    "\n",
    "Modify these parameters to experiment with different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5350dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úèÔ∏è Updated Configuration:\n",
      "   Model Size: d_model=32, d_ff=64, heads=4, layers=2\n",
      "   TimesNet: top_k=3, kernels=3\n",
      "   Training: epochs=10, batch=32, lr=0.001\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# TWEAKABLE PARAMETERS - EXPERIMENT\n",
    "# ================================\n",
    "\n",
    "# Modify these for quick experiments:\n",
    "\n",
    "# --- Sequence parameters (affect model complexity and data usage) ---\n",
    "args.seq_len = 50          # Try: 30, 50, 100 (longer = more context, slower training)\n",
    "args.pred_len = 5          # Try: 3, 5, 10 (longer = harder prediction task)\n",
    "\n",
    "# --- Model size (affect memory usage and training time) ---\n",
    "args.d_model = 32          # Try: 16, 32, 64 (larger = more capacity, slower)\n",
    "args.d_ff = 64             # Try: 32, 64, 128 (usually 2x d_model)\n",
    "args.n_heads = 4           # Try: 2, 4, 8 (must divide d_model evenly)\n",
    "args.e_layers = 2          # Try: 1, 2, 3 (more layers = deeper model)\n",
    "\n",
    "# --- TimesNet specific ---\n",
    "args.top_k = 3             # Try: 2, 3, 5 (more frequencies = more complex patterns)\n",
    "args.num_kernels = 3       # Try: 2, 3, 6 (more kernels = more feature extraction)\n",
    "\n",
    "# --- Training parameters ---\n",
    "args.train_epochs = 10     # Try: 5, 10, 20\n",
    "args.batch_size = 32       # Try: 16, 32, 64 (larger = faster but more memory)\n",
    "args.learning_rate = 0.001 # Try: 0.0001, 0.001, 0.01\n",
    "\n",
    "# --- Advanced tweaks ---\n",
    "args.dropout = 0.1         # Try: 0.0, 0.1, 0.2 (higher = more regularization)\n",
    "args.moving_avg = 25       # Try: 15, 25, 50 (window for trend decomposition)\n",
    "\n",
    "print(f\"‚úèÔ∏è Updated Configuration:\")\n",
    "print(f\"   Model Size: d_model={args.d_model}, d_ff={args.d_ff}, heads={args.n_heads}, layers={args.e_layers}\")\n",
    "print(f\"   TimesNet: top_k={args.top_k}, kernels={args.num_kernels}\")\n",
    "print(f\"   Training: epochs={args.train_epochs}, batch={args.batch_size}, lr={args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a24e9e",
   "metadata": {},
   "source": [
    "## üöÄ Training Setup and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ba968bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Selected Device: cpu\n",
      "‚ö†Ô∏è  Running on CPU - training will be slower\n",
      "üí° Tips: Install CUDA-compatible PyTorch for GPU acceleration\n",
      "üî• Using device: cpu\n",
      "üìÅ Checkpoints: ./checkpoints/TimesNet_light_20250616_1010\n",
      "2025-06-16 10:10:10,221 [INFO] TSLib: Initializing Dataset_Custom with targets: log_Close\n",
      "2025-06-16 10:10:10,456 [INFO] TSLib: Border calculation: n=7109, s=50, p=5, v=10, t=10\n",
      "2025-06-16 10:10:10,457 [INFO] TSLib: border1s = [0, 7039, 7049]\n",
      "2025-06-16 10:10:10,459 [INFO] TSLib: border2s = [7089, 7099, 7109]\n",
      "2025-06-16 10:10:10,456 [INFO] TSLib: Border calculation: n=7109, s=50, p=5, v=10, t=10\n",
      "2025-06-16 10:10:10,457 [INFO] TSLib: border1s = [0, 7039, 7049]\n",
      "2025-06-16 10:10:10,459 [INFO] TSLib: border2s = [7089, 7099, 7109]\n",
      "2025-06-16 10:10:10,494 [INFO] TSLib: Loaded data shape: (7109, 119)\n",
      "2025-06-16 10:10:10,495 [INFO] TSLib: Data_x shape: (7089, 118), Data_y shape: (7089, 118)\n",
      "2025-06-16 10:10:10,495 [INFO] TSLib: Initializing Dataset_Custom with targets: log_Close\n",
      "2025-06-16 10:10:10,494 [INFO] TSLib: Loaded data shape: (7109, 119)\n",
      "2025-06-16 10:10:10,495 [INFO] TSLib: Data_x shape: (7089, 118), Data_y shape: (7089, 118)\n",
      "2025-06-16 10:10:10,495 [INFO] TSLib: Initializing Dataset_Custom with targets: log_Close\n",
      "2025-06-16 10:10:10,735 [INFO] TSLib: Border calculation: n=7109, s=50, p=5, v=10, t=10\n",
      "2025-06-16 10:10:10,735 [INFO] TSLib: border1s = [0, 7039, 7049]\n",
      "2025-06-16 10:10:10,749 [INFO] TSLib: border2s = [7089, 7099, 7109]\n",
      "2025-06-16 10:10:10,735 [INFO] TSLib: Border calculation: n=7109, s=50, p=5, v=10, t=10\n",
      "2025-06-16 10:10:10,735 [INFO] TSLib: border1s = [0, 7039, 7049]\n",
      "2025-06-16 10:10:10,749 [INFO] TSLib: border2s = [7089, 7099, 7109]\n",
      "2025-06-16 10:10:10,773 [INFO] TSLib: Loaded data shape: (7109, 119)\n",
      "2025-06-16 10:10:10,773 [INFO] TSLib: Data_x shape: (60, 118), Data_y shape: (60, 118)\n",
      "2025-06-16 10:10:10,773 [INFO] TSLib: Initializing Dataset_Custom with targets: log_Close\n",
      "2025-06-16 10:10:10,773 [INFO] TSLib: Loaded data shape: (7109, 119)\n",
      "2025-06-16 10:10:10,773 [INFO] TSLib: Data_x shape: (60, 118), Data_y shape: (60, 118)\n",
      "2025-06-16 10:10:10,773 [INFO] TSLib: Initializing Dataset_Custom with targets: log_Close\n",
      "2025-06-16 10:10:11,002 [INFO] TSLib: Border calculation: n=7109, s=50, p=5, v=10, t=10\n",
      "2025-06-16 10:10:11,002 [INFO] TSLib: border1s = [0, 7039, 7049]\n",
      "2025-06-16 10:10:11,002 [INFO] TSLib: border2s = [7089, 7099, 7109]\n",
      "2025-06-16 10:10:11,002 [INFO] TSLib: Border calculation: n=7109, s=50, p=5, v=10, t=10\n",
      "2025-06-16 10:10:11,002 [INFO] TSLib: border1s = [0, 7039, 7049]\n",
      "2025-06-16 10:10:11,002 [INFO] TSLib: border2s = [7089, 7099, 7109]\n",
      "2025-06-16 10:10:11,036 [INFO] TSLib: Loaded data shape: (7109, 119)\n",
      "2025-06-16 10:10:11,036 [INFO] TSLib: Data_x shape: (60, 118), Data_y shape: (60, 118)\n",
      "üìä Data loaders created:\n",
      "   Train: 219 batches\n",
      "   Val: 0 batches\n",
      "   Test: 0 batches\n",
      "2025-06-16 10:10:11,036 [INFO] TSLib: Loaded data shape: (7109, 119)\n",
      "2025-06-16 10:10:11,036 [INFO] TSLib: Data_x shape: (60, 118), Data_y shape: (60, 118)\n",
      "üìä Data loaders created:\n",
      "   Train: 219 batches\n",
      "   Val: 0 batches\n",
      "   Test: 0 batches\n"
     ]
    }
   ],
   "source": [
    "# Setup device and create checkpoint directory\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Enhanced device information\n",
    "print(f\"üéØ Selected Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    print(\"‚ö° Parallel processing: ENABLED (automatic)\")\n",
    "    print(\"üí° Tips: Model will automatically use GPU cores for faster training\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Running on CPU - training will be slower\")\n",
    "    print(\"üí° Tips: Install CUDA-compatible PyTorch for GPU acceleration\")\n",
    "os.makedirs(args.checkpoints, exist_ok=True)\n",
    "\n",
    "print(f\"üî• Using device: {device}\")\n",
    "print(f\"üìÅ Checkpoints: {args.checkpoints}\")\n",
    "\n",
    "# Data loader setup\n",
    "def create_data_loader(flag):\n",
    "    args.validation_length = args.val_len\n",
    "    args.test_length = args.test_len\n",
    "    \n",
    "    dataset = Dataset_Custom(\n",
    "        args=args,\n",
    "        root_path=args.root_path,\n",
    "        data_path=args.data_path,\n",
    "        flag=flag,\n",
    "        size=[args.seq_len, args.label_len, args.pred_len],\n",
    "        features=args.features,\n",
    "        target=args.target,\n",
    "        scale=True,\n",
    "        timeenc=1 if args.embed == 'timeF' else 0,\n",
    "        freq=args.freq\n",
    "    )\n",
    "    \n",
    "    shuffle = (flag == 'train')\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = create_data_loader('train')\n",
    "val_loader = create_data_loader('val')\n",
    "test_loader = create_data_loader('test')\n",
    "\n",
    "print(f\"üìä Data loaders created:\")\n",
    "print(f\"   Train: {len(train_loader)} batches\")\n",
    "print(f\"   Val: {len(val_loader)} batches\")\n",
    "print(f\"   Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa13d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing data loading performance...\n",
      "üìä Testing Data Loading Speed...\n",
      "------------------------------\n",
      "1Ô∏è‚É£ Testing Dataset Creation...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Dataset_Custom.__init__() missing 1 required positional argument: 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Run data loading test\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müß™ Testing data loading performance...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m data_load_speed = \u001b[43mtest_data_loading_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtest_data_loading_speed\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m ds_start = time.time()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Create dataset using the same approach as the working data loader\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m test_data_set = \u001b[43mDataset_Custom\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabel_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpred_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfreq\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m ds_time = time.time() - ds_start\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚è±Ô∏è Dataset creation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Dataset_Custom.__init__() missing 1 required positional argument: 'args'"
     ]
    }
   ],
   "source": [
    "# üìä DATA LOADING SPEED TEST\n",
    "# Test if the bottleneck is in data loading itself\n",
    "\n",
    "def test_data_loading_speed():\n",
    "    \"\"\"Test data loading performance independently\"\"\"\n",
    "    print(\"üìä Testing Data Loading Speed...\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Since we're using the standard TSLib approach, test Dataset_Custom directly\n",
    "    print(\"1Ô∏è‚É£ Testing Dataset Creation...\")\n",
    "    ds_start = time.time()\n",
    "    \n",
    "    # Create dataset using the same approach as the working data loader\n",
    "    test_data_set = Dataset_Custom(\n",
    "        args=args,\n",
    "        root_path=args.root_path,\n",
    "        data_path=args.data_path,\n",
    "        flag='train',\n",
    "        size=[args.seq_len, args.label_len, args.pred_len],\n",
    "        features=args.features,\n",
    "        target=args.target,\n",
    "        scale=True,\n",
    "        timeenc=1 if args.embed == 'timeF' else 0,\n",
    "        freq=args.freq\n",
    "    )\n",
    "    \n",
    "    ds_time = time.time() - ds_start\n",
    "    print(f\"   ‚è±Ô∏è Dataset creation: {ds_time:.3f}s\")\n",
    "    print(f\"   üìà Train samples: {len(test_data_set)}\")\n",
    "    \n",
    "    # Test dataloader creation\n",
    "    print(\"2Ô∏è‚É£ Testing DataLoader Creation...\")\n",
    "    dl_start = time.time()\n",
    "    test_train_loader = DataLoader(\n",
    "        test_data_set, \n",
    "        batch_size=args.batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True\n",
    "    )\n",
    "    dl_time = time.time() - dl_start\n",
    "    print(f\"   ‚è±Ô∏è DataLoader creation: {dl_time:.3f}s\")\n",
    "    print(f\"   üìä Number of batches: {len(test_train_loader)}\")\n",
    "    \n",
    "    # Test batch loading speed\n",
    "    print(\"3Ô∏è‚É£ Testing Batch Loading Speed...\")\n",
    "    batch_times = []\n",
    "    \n",
    "    # Test first 5 batches\n",
    "    test_iter = iter(test_train_loader)\n",
    "    for i in range(min(5, len(test_train_loader))):\n",
    "        batch_start = time.time()\n",
    "        batch_x, batch_y, batch_x_mark, batch_y_mark = next(test_iter)\n",
    "        batch_time = time.time() - batch_start\n",
    "        batch_times.append(batch_time)\n",
    "        print(f\"   Batch {i+1}: {batch_time:.3f}s - Shape: {batch_x.shape}\")\n",
    "    \n",
    "    avg_batch_load = sum(batch_times) / len(batch_times)\n",
    "    print(f\"\\nüìä Data Loading Results:\")\n",
    "    print(f\"   ‚è±Ô∏è Average batch load time: {avg_batch_load:.3f}s\")\n",
    "    print(f\"   üöÄ Est. total data load time: {avg_batch_load * len(test_train_loader):.1f}s\")\n",
    "    \n",
    "    if avg_batch_load > 0.1:\n",
    "        print(\"   ‚ö†Ô∏è Data loading is slow! Recommendations:\")\n",
    "        print(\"   üí° Reduce num_workers to 0\")\n",
    "        print(\"   üí° Check if data file is corrupted\")\n",
    "        print(\"   üí° Consider using smaller batch size\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Data loading speed looks good!\")\n",
    "    \n",
    "    return avg_batch_load\n",
    "\n",
    "# Run data loading test\n",
    "print(\"üß™ Testing data loading performance...\")\n",
    "data_load_speed = test_data_loading_speed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46143ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TimesNet model\n",
    "model = TimesNet(args).to(device)\n",
    "\n",
    "# Setup training components\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
    "\n",
    "# Model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"üß† TimesNet Light Model Initialized:\")\n",
    "print(f\"   üìä Total Parameters: {total_params:,}\")\n",
    "print(f\"   üéØ Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"   üíæ Model Size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ad3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with progress tracking\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"üèÉ Training on {num_batches} batches...\")\n",
    "    \n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        \n",
    "        # Prepare decoder input\n",
    "        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        \n",
    "        # Calculate loss (only on target columns - first 4 features)\n",
    "        target_outputs = outputs[:, -args.pred_len:, :4]\n",
    "        target_y = batch_y[:, -args.pred_len:, :4]\n",
    "        loss = criterion(target_outputs, target_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Progress reporting - EVERY BATCH for Light Config\n",
    "        progress_pct = (i + 1) / num_batches * 100\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        remaining = elapsed / (i + 1) * (num_batches - i - 1)\n",
    "        \n",
    "        # Show progress for every batch (Light config)\n",
    "        print(f\"   üìä Batch {i+1:3d}/{num_batches} ({progress_pct:5.1f}%) - \"\n",
    "              f\"Loss: {loss.item():.6f} (Avg: {avg_loss:.6f}) - \"\n",
    "              f\"‚è±Ô∏è Elapsed: {elapsed:.1f}s, ETA: {remaining:.1f}s\")\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"‚úÖ Epoch completed in {epoch_time:.1f}s. Average loss: {avg_loss:.6f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in val_loader:\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "            \n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "            \n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            \n",
    "            target_outputs = outputs[:, -args.pred_len:, :4]\n",
    "            target_y = batch_y[:, -args.pred_len:, :4]\n",
    "            loss = criterion(target_outputs, target_y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "print(\"üîß Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b56cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(f\"üöÄ Starting TimesNet Light Training ({args.train_epochs} epochs)\")\n",
    "print(f\"‚è∞ Estimated time: ~{args.train_epochs * 5} minutes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(args.train_epochs):\n",
    "    print(f\"\\nüîÑ Epoch {epoch+1}/{args.train_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    print(\"üîç Running validation...\")\n",
    "    val_loss = validate_epoch()\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Log progress\n",
    "    print(f\"üìà Epoch {epoch+1} Results: Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    # Adjust learning rate\n",
    "    adjust_learning_rate(optimizer, epoch + 1, args)\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping(val_loss, model, args.checkpoints)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"‚èπÔ∏è Early stopping triggered\")\n",
    "        break\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f\"{args.checkpoints}/best_model.pth\")\n",
    "        print(f\"üíæ New best model saved (Val Loss: {val_loss:.6f})\")\n",
    "\n",
    "total_training_time = time.time() - training_start_time\n",
    "print(f\"\\nüéâ Training completed in {total_training_time/60:.1f} minutes!\")\n",
    "print(f\"üèÜ Best validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f24488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç COMPREHENSIVE PERFORMANCE DIAGNOSTICS\n",
    "# This cell will help identify exactly where the bottleneck is occurring\n",
    "\n",
    "import time\n",
    "import torch.profiler\n",
    "\n",
    "def diagnose_training_bottleneck():\n",
    "    \"\"\"Run comprehensive diagnostics to identify training bottlenecks\"\"\"\n",
    "    print(\"üîç Running Training Performance Diagnostics...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Test data loading speed\n",
    "    print(\"1Ô∏è‚É£ Testing Data Loading Speed...\")\n",
    "    data_start = time.time()\n",
    "    train_iter = iter(train_loader)\n",
    "    batch_x, batch_y, batch_x_mark, batch_y_mark = next(train_iter)\n",
    "    data_load_time = time.time() - data_start\n",
    "    print(f\"   ‚è±Ô∏è First batch load time: {data_load_time:.3f}s\")\n",
    "    \n",
    "    # 2. Test data transfer to device\n",
    "    print(\"\\n2Ô∏è‚É£ Testing Data Transfer to Device...\")\n",
    "    transfer_start = time.time()\n",
    "    batch_x = batch_x.float().to(device)\n",
    "    batch_y = batch_y.float().to(device)\n",
    "    batch_x_mark = batch_x_mark.float().to(device)\n",
    "    batch_y_mark = batch_y_mark.float().to(device)\n",
    "    transfer_time = time.time() - transfer_start\n",
    "    print(f\"   ‚è±Ô∏è Device transfer time: {transfer_time:.3f}s\")\n",
    "    \n",
    "    # 3. Test decoder input preparation\n",
    "    print(\"\\n3Ô∏è‚É£ Testing Decoder Input Preparation...\")\n",
    "    prep_start = time.time()\n",
    "    dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "    dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "    prep_time = time.time() - prep_start\n",
    "    print(f\"   ‚è±Ô∏è Decoder prep time: {prep_time:.3f}s\")\n",
    "    \n",
    "    # 4. Test model forward pass\n",
    "    print(\"\\n4Ô∏è‚É£ Testing Model Forward Pass...\")\n",
    "    model.eval()  # Set to eval for consistent timing\n",
    "    forward_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "    forward_time = time.time() - forward_start\n",
    "    print(f\"   ‚è±Ô∏è Forward pass time: {forward_time:.3f}s\")\n",
    "    \n",
    "    # 5. Test loss calculation\n",
    "    print(\"\\n5Ô∏è‚É£ Testing Loss Calculation...\")\n",
    "    loss_start = time.time()\n",
    "    target_outputs = outputs[:, -args.pred_len:, :4]\n",
    "    target_y = batch_y[:, -args.pred_len:, :4]\n",
    "    loss = criterion(target_outputs, target_y)\n",
    "    loss_time = time.time() - loss_start\n",
    "    print(f\"   ‚è±Ô∏è Loss calculation time: {loss_time:.3f}s\")\n",
    "    \n",
    "    # 6. Test backward pass\n",
    "    print(\"\\n6Ô∏è‚É£ Testing Backward Pass...\")\n",
    "    model.train()  # Set back to training mode\n",
    "    # Create fresh batch for backward pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "    target_outputs = outputs[:, -args.pred_len:, :4]\n",
    "    target_y = batch_y[:, -args.pred_len:, :4]\n",
    "    loss = criterion(target_outputs, target_y)\n",
    "    \n",
    "    backward_start = time.time()\n",
    "    loss.backward()\n",
    "    backward_time = time.time() - backward_start\n",
    "    print(f\"   ‚è±Ô∏è Backward pass time: {backward_time:.3f}s\")\n",
    "    \n",
    "    # 7. Test optimizer step\n",
    "    print(\"\\n7Ô∏è‚É£ Testing Optimizer Step...\")\n",
    "    step_start = time.time()\n",
    "    optimizer.step()\n",
    "    step_time = time.time() - step_start\n",
    "    print(f\"   ‚è±Ô∏è Optimizer step time: {step_time:.3f}s\")\n",
    "    \n",
    "    # 8. Calculate total time per batch\n",
    "    total_time = data_load_time + transfer_time + prep_time + forward_time + loss_time + backward_time + step_time\n",
    "    print(f\"\\nüìä SUMMARY:\")\n",
    "    print(f\"   üî¢ Total time per batch: {total_time:.3f}s\")\n",
    "    print(f\"   üèÉ Expected time for {len(train_loader)} batches: {total_time * len(train_loader):.1f}s ({total_time * len(train_loader)/60:.1f} min)\")\n",
    "    \n",
    "    # Identify bottlenecks\n",
    "    times = {\n",
    "        'Data Loading': data_load_time,\n",
    "        'Device Transfer': transfer_time,\n",
    "        'Decoder Prep': prep_time,\n",
    "        'Forward Pass': forward_time,\n",
    "        'Loss Calculation': loss_time,\n",
    "        'Backward Pass': backward_time,\n",
    "        'Optimizer Step': step_time\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüéØ BOTTLENECK ANALYSIS:\")\n",
    "    sorted_times = sorted(times.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i, (operation, op_time) in enumerate(sorted_times):\n",
    "        percentage = (op_time / total_time) * 100\n",
    "        symbol = \"üî¥\" if i == 0 else \"üü°\" if i == 1 else \"üü¢\"\n",
    "        print(f\"   {symbol} {operation}: {op_time:.3f}s ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nüíæ GPU Memory Usage:\")\n",
    "        print(f\"   üìä Allocated: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")\n",
    "        print(f\"   üìà Reserved: {torch.cuda.memory_reserved()/1024**2:.1f} MB\")\n",
    "    \n",
    "    return total_time\n",
    "\n",
    "# Run diagnostics\n",
    "single_batch_time = diagnose_training_bottleneck()\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "if single_batch_time > 2.0:\n",
    "    print(\"   ‚ö†Ô∏è Training is slower than expected. Main issues likely:\")\n",
    "    print(\"   1. Model is too complex for ultra-fast config\")\n",
    "    print(\"   2. Data loading bottleneck\")\n",
    "    print(\"   3. GPU memory issues\")\n",
    "elif single_batch_time > 0.5:\n",
    "    print(\"   üü° Training speed is acceptable but could be faster\")\n",
    "    print(\"   üí° Consider reducing model complexity further\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Training speed looks good!\")\n",
    "    print(\"   üöÄ Full training should complete quickly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4425260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° MICRO-BENCHMARK - Quick Speed Test\n",
    "# Run this first to get immediate feedback on performance\n",
    "\n",
    "def quick_speed_test():\n",
    "    \"\"\"Quick 10-batch speed test to identify obvious bottlenecks\"\"\"\n",
    "    print(\"‚ö° Running Quick Speed Test (10 batches)...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    model.train()\n",
    "    test_batches = min(10, len(train_loader))\n",
    "    \n",
    "    # Warm-up\n",
    "    print(\"üî• Warming up...\")\n",
    "    train_iter = iter(train_loader)\n",
    "    batch_x, batch_y, batch_x_mark, batch_y_mark = next(train_iter)\n",
    "    batch_x = batch_x.float().to(device)\n",
    "    batch_y = batch_y.float().to(device)\n",
    "    batch_x_mark = batch_x_mark.float().to(device)\n",
    "    batch_y_mark = batch_y_mark.float().to(device)\n",
    "    \n",
    "    # Quick forward pass for warm-up\n",
    "    dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "    dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "    \n",
    "    print(\"‚úÖ Warm-up complete. Starting speed test...\")\n",
    "    \n",
    "    # Test loop\n",
    "    total_start = time.time()\n",
    "    batch_times = []\n",
    "    \n",
    "    train_iter = iter(train_loader)  # Fresh iterator\n",
    "    \n",
    "    for i in range(test_batches):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        # Get batch\n",
    "        batch_x, batch_y, batch_x_mark, batch_y_mark = next(train_iter)\n",
    "        \n",
    "        # Move to device\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        \n",
    "        # Prepare decoder input\n",
    "        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        \n",
    "        # Calculate loss\n",
    "        target_outputs = outputs[:, -args.pred_len:, :4]\n",
    "        target_y = batch_y[:, -args.pred_len:, :4]\n",
    "        loss = criterion(target_outputs, target_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        batch_times.append(batch_time)\n",
    "        \n",
    "        print(f\"   Batch {i+1:2d}: {batch_time:.3f}s - Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    total_time = time.time() - total_start\n",
    "    avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "    \n",
    "    print(f\"\\nüìä SPEED TEST RESULTS:\")\n",
    "    print(f\"   ‚è±Ô∏è Total time: {total_time:.2f}s\")\n",
    "    print(f\"   üìà Average per batch: {avg_batch_time:.3f}s\")\n",
    "    print(f\"   üöÄ Estimated full training time: {avg_batch_time * len(train_loader) * args.train_epochs / 60:.1f} minutes\")\n",
    "    \n",
    "    # Classification\n",
    "    if avg_batch_time < 0.1:\n",
    "        print(\"   ‚úÖ EXCELLENT: Training should be very fast!\")\n",
    "    elif avg_batch_time < 0.5:\n",
    "        print(\"   üü¢ GOOD: Training speed looks acceptable\")\n",
    "    elif avg_batch_time < 2.0:\n",
    "        print(\"   üü° MODERATE: Training will be slower than expected\")\n",
    "    else:\n",
    "        print(\"   üî¥ SLOW: Major bottleneck detected!\")\n",
    "        print(\"   üí° Consider reducing model size or batch size\")\n",
    "    \n",
    "    return avg_batch_time\n",
    "\n",
    "# Run quick speed test\n",
    "print(\"üèÉ‚Äç‚ôÄÔ∏è Starting quick speed test...\")\n",
    "avg_time = quick_speed_test()\n",
    "\n",
    "# Conditional message\n",
    "if avg_time > 1.0:\n",
    "    print(f\"\\n‚ö†Ô∏è SPEED ISSUE DETECTED!\")\n",
    "    print(f\"   üìä Current speed: {avg_time:.3f}s per batch\")\n",
    "    print(f\"   üéØ Target speed: <0.5s per batch for ultra-fast config\")\n",
    "    print(f\"   üí° Run the full diagnostics above to identify the bottleneck\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Speed looks good! Proceeding with training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd577d30",
   "metadata": {},
   "source": [
    "## üîß **TROUBLESHOOTING GUIDE - If Training is Too Slow**\n",
    "\n",
    "If you're experiencing slow training performance, follow these steps in order:\n",
    "\n",
    "### üìã **Step-by-Step Debugging Process:**\n",
    "\n",
    "**1. üö® First: Try Debugging Mode**\n",
    "```python\n",
    "# Switch to the most minimal configuration possible\n",
    "switch_to_debugging()\n",
    "```\n",
    "\n",
    "**2. ‚ö° Quick Speed Test**\n",
    "- Run the \"Quick Speed Test\" cell above\n",
    "- This will tell you immediately if there's a major bottleneck\n",
    "- Target: <0.5s per batch for ultra-fast config\n",
    "\n",
    "**3. üìä Data Loading Test**\n",
    "- Run the \"Data Loading Speed Test\" cell\n",
    "- This isolates data loading performance\n",
    "- Target: <0.1s per batch for data loading\n",
    "\n",
    "**4. üîç Full Diagnostics**\n",
    "- Run the \"Comprehensive Performance Diagnostics\" cell\n",
    "- This will show you exactly where the bottleneck is\n",
    "- Look for the üî¥ red items in the bottleneck analysis\n",
    "\n",
    "**5. üõ†Ô∏è Common Solutions:**\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| üî¥ Data Loading Slow | Set `num_workers = 0` |\n",
    "| üî¥ Forward Pass Slow | Use smaller `d_model` or fewer layers |\n",
    "| üî¥ Memory Issues | Reduce `batch_size` |\n",
    "| üî¥ GPU Not Used | Check CUDA availability |\n",
    "| üî¥ Model Too Complex | Switch to debugging config |\n",
    "\n",
    "**6. üéØ Performance Targets:**\n",
    "\n",
    "| Configuration | Target Time/Batch | Total Training Time |\n",
    "|---------------|-------------------|-------------------|\n",
    "| üö® **Debugging** | **<0.1s** | **<30 seconds** |\n",
    "| ‚ö° **Ultra-Fast** | **<0.5s** | **<2 minutes** |\n",
    "| üí° **Light** | <1.0s | <10 minutes |\n",
    "\n",
    "### üí° **Quick Fixes to Try:**\n",
    "\n",
    "1. **Switch to debugging mode** (most important)\n",
    "2. **Disable multiprocessing**: Set `num_workers = 0`\n",
    "3. **Disable AMP**: Set `use_amp = False`\n",
    "4. **Reduce batch size**: Try `batch_size = 8`\n",
    "5. **Check GPU usage**: Ensure CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ca311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üö® EMERGENCY MODE - Run this if everything else is too slow!\n",
    "\n",
    "def emergency_debug_mode():\n",
    "    \"\"\"\n",
    "    Ultra-minimal test that should complete in under 30 seconds\n",
    "    This will tell us if the basic training loop works at all\n",
    "    \"\"\"\n",
    "    print(\"üö® EMERGENCY DEBUG MODE ACTIVATED!\")\n",
    "    print(\"üéØ Goal: Complete 1 epoch with minimal model in <30 seconds\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Switch to debugging config\n",
    "    switch_to_debugging()\n",
    "    \n",
    "    # Quick data setup\n",
    "    print(\"üìä Setting up minimal data...\")\n",
    "    \n",
    "    # Use the same data loading approach as the working parts\n",
    "    emergency_data_set = Dataset_Custom(\n",
    "        args=args,\n",
    "        root_path=args.root_path,\n",
    "        data_path=args.data_path,\n",
    "        flag='train',\n",
    "        size=[args.seq_len, args.label_len, args.pred_len],\n",
    "        features=args.features,\n",
    "        target=args.target,\n",
    "        scale=True,\n",
    "        timeenc=1 if args.embed == 'timeF' else 0,\n",
    "        freq=args.freq\n",
    "    )\n",
    "    \n",
    "    # Create minimal dataloader\n",
    "    emergency_train_loader = DataLoader(\n",
    "        emergency_data_set, \n",
    "        batch_size=args.batch_size, \n",
    "        shuffle=False,  # No shuffle for speed\n",
    "        num_workers=0,  # No multiprocessing\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    print(f\"üìà Emergency setup: {len(emergency_train_loader)} batches\")\n",
    "    \n",
    "    # Create minimal model\n",
    "    print(\"üß† Creating minimal model...\")\n",
    "    from models import TimesNet\n",
    "    emergency_model = TimesNet.Model(args).float().to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    emergency_params = sum(p.numel() for p in emergency_model.parameters())\n",
    "    print(f\"üî¢ Emergency model: {emergency_params:,} parameters\")\n",
    "    \n",
    "    # Setup training\n",
    "    emergency_optimizer = torch.optim.Adam(emergency_model.parameters(), lr=args.learning_rate)\n",
    "    emergency_criterion = nn.MSELoss()\n",
    "    \n",
    "    # Run emergency training\n",
    "    print(\"üöÄ Starting emergency training...\")\n",
    "    emergency_start = time.time()\n",
    "    \n",
    "    emergency_model.train()\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(emergency_train_loader):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        # Move to device\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        \n",
    "        # Prepare decoder input\n",
    "        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        emergency_optimizer.zero_grad()\n",
    "        outputs = emergency_model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        \n",
    "        # Calculate loss\n",
    "        target_outputs = outputs[:, -args.pred_len:, :4]\n",
    "        target_y = batch_y[:, -args.pred_len:, :4]\n",
    "        loss = emergency_criterion(target_outputs, target_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        emergency_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        print(f\"   ‚ö° Batch {i+1}: {batch_time:.3f}s - Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        # Stop after 5 batches or if too slow\n",
    "        if i >= 4 or batch_time > 2.0:\n",
    "            if batch_time > 2.0:\n",
    "                print(f\"   üî¥ STOPPING: Batch too slow ({batch_time:.1f}s)\")\n",
    "            break\n",
    "    \n",
    "    emergency_time = time.time() - emergency_start\n",
    "    avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä EMERGENCY TEST RESULTS:\")\n",
    "    print(f\"   ‚è±Ô∏è Total time: {emergency_time:.1f}s\")\n",
    "    print(f\"   üìà Batches completed: {batch_count}\")\n",
    "    print(f\"   üíØ Average loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    if emergency_time < 30:\n",
    "        print(\"   ‚úÖ SUCCESS: Basic training works!\")\n",
    "        print(\"   üí° The issue may be with your larger configuration\")\n",
    "    else:\n",
    "        print(\"   üî¥ MAJOR ISSUE: Even minimal training is too slow\")\n",
    "        print(\"   üí° Check GPU availability, data corruption, or system resources\")\n",
    "    \n",
    "    return emergency_time < 30\n",
    "\n",
    "print(\"üö® EMERGENCY MODE AVAILABLE\")\n",
    "print(\"üí° Run emergency_debug_mode() if all other configs are too slow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab72e30",
   "metadata": {},
   "source": [
    "## üìä Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and test\n",
    "model.load_state_dict(torch.load(f\"{args.checkpoints}/best_model.pth\", weights_only=False))\n",
    "model.eval()\n",
    "\n",
    "# Test evaluation\n",
    "preds = []\n",
    "trues = []\n",
    "\n",
    "print(\"üß™ Testing model...\")\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y, batch_x_mark, batch_y_mark in test_loader:\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        \n",
    "        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "        \n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        \n",
    "        pred = outputs[:, -args.pred_len:, :4].detach().cpu().numpy()\n",
    "        true = batch_y[:, -args.pred_len:, :4].detach().cpu().numpy()\n",
    "        \n",
    "        preds.append(pred)\n",
    "        trues.append(true)\n",
    "\n",
    "# Calculate metrics\n",
    "if preds:\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    \n",
    "    mae, mse, rmse, mape, mspe = metric(preds, trues)\n",
    "    \n",
    "    print(\"\\nüìä TimesNet Light - Test Results:\")\n",
    "    print(f\"   üéØ MSE:  {mse:.6f}\")\n",
    "    print(f\"   üìè MAE:  {mae:.6f}\")\n",
    "    print(f\"   üìê RMSE: {rmse:.6f}\")\n",
    "    print(f\"   üìà MAPE: {mape:.6f}%\")\n",
    "    print(f\"   üìâ MSPE: {mspe:.6f}%\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nüìã Configuration Summary:\")\n",
    "    print(f\"   ‚ö° Model: Light ({total_params:,} params)\")\n",
    "    print(f\"   üìè Sequence: {args.seq_len} ‚Üí {args.pred_len}\")\n",
    "    print(f\"   üß† Architecture: d_model={args.d_model}, layers={args.e_layers}, heads={args.n_heads}\")\n",
    "    print(f\"   ‚è±Ô∏è Training time: {total_training_time/60:.1f} minutes\")\n",
    "    print(f\"   üèÜ Final performance: RMSE={rmse:.6f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No test data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Enhanced Progress Monitoring for Light Config\n",
    "print(\"‚ö° Light Configuration - Enhanced Batch Monitoring Enabled\")\n",
    "print(\"üìä Every batch will show detailed progress for faster feedback\")\n",
    "print(\"üí° This helps track training progress in real-time\")\n",
    "print()\n",
    "\n",
    "# Override the train_epoch function with detailed batch printing\n",
    "def train_epoch_verbose():\n",
    "    \"\"\"Enhanced training function with per-batch progress\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"üèÉ Training on {num_batches} batches with DETAILED progress...\")\n",
    "    print(f\"üìä Batch format: [Batch X/Y (Z%)] - Loss: current (average) - Time: elapsed/remaining\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        # Move to device\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "        \n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        target_outputs = outputs[:, -args.pred_len:, :4]\n",
    "        target_y = batch_y[:, -args.pred_len:, :4]\n",
    "        loss = criterion(target_outputs, target_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_time = time.time() - batch_start\n",
    "        \n",
    "        # DETAILED progress for every batch\n",
    "        progress_pct = (i + 1) / num_batches * 100\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        remaining = elapsed / (i + 1) * (num_batches - i - 1)\n",
    "        \n",
    "        # Progress bar visualization\n",
    "        bar_length = 20\n",
    "        filled_length = int(bar_length * (i + 1) // num_batches)\n",
    "        bar = '‚ñà' * filled_length + '-' * (bar_length - filled_length)\n",
    "        \n",
    "        print(f\"   [{bar}] Batch {i+1:3d}/{num_batches} ({progress_pct:5.1f}%) - \"\n",
    "              f\"Loss: {loss.item():.6f} (Avg: {avg_loss:.6f}) - \"\n",
    "              f\"Time: {elapsed:.1f}s/{remaining:.1f}s - \"\n",
    "              f\"Batch: {batch_time:.2f}s\")\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"‚úÖ Epoch completed in {epoch_time:.1f}s. Average loss: {avg_loss:.6f}\")\n",
    "    print(f\"‚ö° Average time per batch: {epoch_time/num_batches:.2f}s\")\n",
    "    return avg_loss\n",
    "\n",
    "print(\"üéØ Enhanced training function ready!\")\n",
    "print(\"üí° Now each batch will show:\")\n",
    "print(\"   - Progress bar visualization\")\n",
    "print(\"   - Current and average loss\")\n",
    "print(\"   - Elapsed and remaining time\")\n",
    "print(\"   - Individual batch processing time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5114a84",
   "metadata": {},
   "source": [
    "## ‚ö° Enhanced Training with Detailed Batch Progress\n",
    "\n",
    "Now you can use the enhanced training function that shows progress for **every single batch**:\n",
    "\n",
    "### üéØ **Features Added:**\n",
    "- **üìä Progress Bar**: Visual progress indicator for each epoch\n",
    "- **‚è±Ô∏è Time Tracking**: Shows elapsed time and estimated remaining time\n",
    "- **üìà Loss Monitoring**: Current batch loss and running average\n",
    "- **üöÄ Batch Timing**: Individual batch processing time\n",
    "\n",
    "### üí° **Why This Helps:**\n",
    "- **No More Waiting**: See progress immediately, no need to wait for epoch completion\n",
    "- **Performance Insights**: Identify if any batches are unusually slow\n",
    "- **Loss Tracking**: Monitor if the model is learning batch by batch\n",
    "- **Time Estimation**: Know exactly when training will complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ RUN ENHANCED TRAINING LOOP WITH DETAILED BATCH PROGRESS\n",
    "print(\"üéØ Starting Enhanced TimesNet Light Training\")\n",
    "print(\"‚ö° Every batch will show detailed progress!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop with enhanced progress\n",
    "for epoch in range(args.train_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print(f\"\\nüî• EPOCH {epoch+1}/{args.train_epochs}\")\n",
    "    print(f\"üìÖ Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    # Enhanced training with detailed batch progress\n",
    "    train_loss = train_epoch_verbose()\n",
    "    \n",
    "    # Validation (standard)\n",
    "    print(f\"\\nüîç Validating...\")\n",
    "    val_loss = validate_epoch()\n",
    "    \n",
    "    # Learning rate adjustment\n",
    "    adjust_learning_rate(optimizer, epoch + 1, args)\n",
    "    \n",
    "    # Record losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    total_elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nüìä EPOCH {epoch+1} SUMMARY:\")\n",
    "    print(f\"   üìà Train Loss: {train_loss:.6f}\")\n",
    "    print(f\"   üìâ Val Loss: {val_loss:.6f}\")\n",
    "    print(f\"   ‚è±Ô∏è Epoch Time: {epoch_time:.1f}s ({epoch_time/60:.1f} min)\")\n",
    "    print(f\"   üïí Total Time: {total_elapsed:.1f}s ({total_elapsed/60:.1f} min)\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    early_stopping(val_loss, model, args.checkpoints)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\n‚èπÔ∏è Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # Estimate remaining time\n",
    "    if epoch < args.train_epochs - 1:\n",
    "        avg_epoch_time = total_elapsed / (epoch + 1)\n",
    "        remaining_epochs = args.train_epochs - (epoch + 1)\n",
    "        estimated_remaining = avg_epoch_time * remaining_epochs\n",
    "        print(f\"   ‚è≥ Estimated remaining time: {estimated_remaining/60:.1f} minutes\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ ENHANCED TRAINING COMPLETED!\")\n",
    "print(f\"‚è∞ Total training time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "print(f\"üìä Final train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"üìâ Final val loss: {val_losses[-1]:.6f}\")\n",
    "print(f\"üèÜ Best val loss: {min(val_losses):.6f} (epoch {val_losses.index(min(val_losses))+1})\")\n",
    "print(\"üí° Check the detailed batch progress above for training insights!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
