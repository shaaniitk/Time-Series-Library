#!/usr/bin/env python3
"""
Visualize the convergence results from synthetic training
"""

import json
import matplotlib.pyplot as plt
import numpy as np

def visualize_convergence():
    """Create a comprehensive convergence visualization"""
    
    # Load results
    with open("logs/synthetic_convergence_20251007_092933.json", 'r') as f:
        results = json.load(f)
    
    train_losses = results['train_losses']
    val_losses = results['val_losses']
    learning_rates = results['learning_rates']
    
    # Create figure
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Enhanced SOTA PGAT - Synthetic Data Convergence Analysis', 
                 fontsize=16, fontweight='bold')
    
    epochs = range(1, len(train_losses) + 1)
    
    # Plot 1: Main convergence curves
    ax1.plot(epochs, train_losses, 'b-o', label='Training Loss', linewidth=3, markersize=8)
    ax1.plot(epochs, val_losses, 'r-s', label='Validation Loss', linewidth=3, markersize=8)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training Convergence', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)
    ax1.set_yscale('log')
    
    # Add annotations for key points
    ax1.annotate(f'Start: {train_losses[0]:.3f}', 
                xy=(1, train_losses[0]), xytext=(2, train_losses[0]*1.5),
                arrowprops=dict(arrowstyle='->', color='blue', alpha=0.7),
                fontsize=10, color='blue')
    
    ax1.annotate(f'Final: {train_losses[-1]:.3f}', 
                xy=(len(train_losses), train_losses[-1]), 
                xytext=(len(train_losses)-1, train_losses[-1]*2),
                arrowprops=dict(arrowstyle='->', color='blue', alpha=0.7),
                fontsize=10, color='blue')
    
    # Plot 2: Improvement percentages
    train_improvements = [(train_losses[0] - loss) / train_losses[0] * 100 for loss in train_losses]
    val_improvements = [(val_losses[0] - loss) / val_losses[0] * 100 for loss in val_losses]
    
    ax2.plot(epochs, train_improvements, 'b-o', label='Training Improvement', linewidth=2, markersize=6)
    ax2.plot(epochs, val_improvements, 'r-s', label='Validation Improvement', linewidth=2, markersize=6)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Improvement (%)', fontsize=12)
    ax2.set_title('Loss Improvement Over Time', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3)
    
    # Add final improvement text
    ax2.text(0.7, 0.9, f'Final Training: {train_improvements[-1]:.1f}%\nFinal Validation: {val_improvements[-1]:.1f}%', 
             transform=ax2.transAxes, fontsize=11, 
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))
    
    # Plot 3: Learning rate schedule
    ax3.plot(epochs, learning_rates, 'g-^', linewidth=2, markersize=6)
    ax3.set_xlabel('Epoch', fontsize=12)
    ax3.set_ylabel('Learning Rate', fontsize=12)
    ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    ax3.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
    
    # Plot 4: Generalization gap
    gaps = [abs(val - train) for train, val in zip(train_losses, val_losses)]
    ax4.plot(epochs, gaps, 'purple', linewidth=2, marker='d', markersize=6)
    ax4.set_xlabel('Epoch', fontsize=12)
    ax4.set_ylabel('|Validation - Training|', fontsize=12)
    ax4.set_title('Generalization Gap', fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    
    # Add gap analysis
    final_gap = gaps[-1]
    min_gap = min(gaps)
    ax4.axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Overfitting Threshold')
    ax4.text(0.6, 0.8, f'Final Gap: {final_gap:.3f}\nMin Gap: {min_gap:.3f}', 
             transform=ax4.transAxes, fontsize=11,
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow", alpha=0.7))
    ax4.legend(fontsize=10)
    
    plt.tight_layout()
    
    # Add summary statistics
    summary_text = f"""
    ğŸ‰ CONVERGENCE SUMMARY ğŸ‰
    Training: {train_losses[0]:.3f} â†’ {train_losses[-1]:.3f} ({train_improvements[-1]:.1f}% improvement)
    Validation: {val_losses[0]:.3f} â†’ {val_losses[-1]:.3f} ({val_improvements[-1]:.1f}% improvement)
    Best Validation: {min(val_losses):.3f} (Epoch {val_losses.index(min(val_losses))+1})
    Final Gap: {final_gap:.3f} (Good generalization)
    """
    
    plt.figtext(0.02, 0.02, summary_text, fontsize=10, 
                bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
    
    # Save plot
    plt.savefig("logs/enhanced_pgat_convergence_analysis.png", dpi=300, bbox_inches='tight')
    print("ğŸ“Š Convergence analysis plot saved to: logs/enhanced_pgat_convergence_analysis.png")
    
    # Print detailed analysis
    print("\nğŸ¯ DETAILED CONVERGENCE ANALYSIS")
    print("=" * 50)
    print(f"ğŸ“ˆ Training Loss: {train_losses[0]:.6f} â†’ {train_losses[-1]:.6f}")
    print(f"ğŸ“Š Validation Loss: {val_losses[0]:.6f} â†’ {val_losses[-1]:.6f}")
    print(f"ğŸ¯ Best Validation: {min(val_losses):.6f} (Epoch {val_losses.index(min(val_losses))+1})")
    print(f"ğŸ“‰ Training Improvement: {train_improvements[-1]:.1f}%")
    print(f"ğŸ“‰ Validation Improvement: {val_improvements[-1]:.1f}%")
    print(f"ğŸ”„ Final Generalization Gap: {final_gap:.6f}")
    
    # Convergence quality assessment
    print(f"\nâœ… CONVERGENCE QUALITY ASSESSMENT")
    print("=" * 50)
    
    if train_improvements[-1] > 80:
        print("ğŸ† Training Convergence: EXCELLENT (>80% improvement)")
    elif train_improvements[-1] > 60:
        print("âœ… Training Convergence: VERY GOOD (>60% improvement)")
    else:
        print("âš ï¸  Training Convergence: MODERATE (<60% improvement)")
    
    if final_gap < 0.05:
        print("ğŸ† Generalization: EXCELLENT (gap < 0.05)")
    elif final_gap < 0.1:
        print("âœ… Generalization: GOOD (gap < 0.1)")
    else:
        print("âš ï¸  Generalization: MONITOR (gap > 0.1)")
    
    if min(val_losses) == val_losses[-1]:
        print("ğŸ† Validation Trend: OPTIMAL (best at end)")
    elif abs(min(val_losses) - val_losses[-1]) < 0.01:
        print("âœ… Validation Trend: STABLE (near optimal)")
    else:
        print("âš ï¸  Validation Trend: EARLY STOPPING NEEDED")
    
    print(f"\nğŸš€ OVERALL ASSESSMENT: EXCELLENT CONVERGENCE!")
    print("The model shows outstanding learning capabilities with stable convergence.")

if __name__ == "__main__":
    visualize_convergence()