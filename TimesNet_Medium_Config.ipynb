{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3909ca44",
   "metadata": {},
   "source": [
    "# TimesNet Medium Configuration - Financial Data Training\n",
    "\n",
    "This notebook contains a **medium TimesNet configuration** optimized for:\n",
    "- Balanced performance and training time\n",
    "- Production-ready experimentation\n",
    "- Good accuracy with reasonable compute requirements\n",
    "- Standard research benchmarking\n",
    "\n",
    "**Dataset**: Financial time series with 4 targets + 114 covariates (118 total features)\n",
    "**Training Time**: ~15-25 minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005596f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "from models.TimesNet import Model as TimesNet\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate\n",
    "from utils.metrics import metric\n",
    "from utils.logger import logger\n",
    "from data_provider.data_loader import Dataset_Custom\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üíª Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e3003",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Medium Configuration Parameters\n",
    "\n",
    "**Purpose**: Balanced performance and efficiency for production use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103497eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# MEDIUM CONFIGURATION - TIMESNET\n",
    "# ================================\n",
    "\n",
    "class MediumConfig:\n",
    "    # === DATA CONFIGURATION ===\n",
    "    data = 'custom'                    # Dataset type (custom for prepared financial data)\n",
    "    root_path = './data/'              # Root directory for data files\n",
    "    data_path = 'prepared_financial_data.csv'  # Main data file\n",
    "    features = 'M'                     # Forecasting mode: 'M'=Multivariate, 'S'=Univariate, 'MS'=Multivariate-to-Univariate\n",
    "    target = 'log_Close'               # Primary target column (for 'S' mode)\n",
    "    freq = 'b'                         # Time frequency: 'b'=business day, 'h'=hourly, 'd'=daily\n",
    "    \n",
    "    # === SEQUENCE PARAMETERS ===\n",
    "    seq_len = 100                      # Input sequence length (lookback window) - MEDIUM: balanced context\n",
    "    label_len = 20                     # Start token length for decoder input (overlap with seq_len)\n",
    "    pred_len = 10                      # Prediction horizon (how many steps to forecast) - MEDIUM: reasonable prediction\n",
    "    \n",
    "    # === TRAIN/VAL/TEST SPLITS ===\n",
    "    val_len = 20                       # Validation set length in time steps\n",
    "    test_len = 20                      # Test set length in time steps\n",
    "    prod_len = 10                      # Production forecast length (future predictions beyond data)\n",
    "    \n",
    "    # === TIMESNET MODEL ARCHITECTURE ===\n",
    "    # Core dimensions\n",
    "    enc_in = 118                       # Encoder input size (total features: 4 targets + 114 covariates)\n",
    "    dec_in = 118                       # Decoder input size (usually same as enc_in)\n",
    "    c_out = 118                        # Output size (must match enc_in to avoid dimension mismatch)\n",
    "    d_model = 64                       # Model dimension (embedding size) - MEDIUM: good capacity\n",
    "    d_ff = 128                         # Feed-forward network dimension - MEDIUM: standard 2x d_model\n",
    "    \n",
    "    # Attention mechanism\n",
    "    n_heads = 8                        # Number of attention heads - MEDIUM: standard attention\n",
    "    e_layers = 3                       # Number of encoder layers - MEDIUM: good depth\n",
    "    d_layers = 1                       # Number of decoder layers (usually 1 for forecasting)\n",
    "    \n",
    "    # TimesNet specific parameters\n",
    "    top_k = 5                          # Top-k frequencies for TimesNet decomposition - MEDIUM: more pattern capture\n",
    "    num_kernels = 6                    # Number of convolution kernels in Inception blocks - MEDIUM: good feature extraction\n",
    "    \n",
    "    # Regularization\n",
    "    dropout = 0.1                      # Dropout rate for regularization\n",
    "    \n",
    "    # Additional model settings\n",
    "    embed = 'timeF'                    # Time feature embedding: 'timeF'=time features, 'fixed'=learnable, 'learned'=learned\n",
    "    activation = 'gelu'                # Activation function: 'gelu', 'relu', 'swish'\n",
    "    factor = 1                         # Attention factor (usually 1)\n",
    "    distil = True                      # Whether to use knowledge distillation\n",
    "    moving_avg = 25                    # Moving average window for trend decomposition\n",
    "    output_attention = False           # Whether to output attention weights (set True for interpretability)\n",
    "    \n",
    "    # === TRAINING CONFIGURATION ===\n",
    "    train_epochs = 20                  # Number of training epochs - MEDIUM: enough for convergence\n",
    "    batch_size = 32                    # Batch size - MEDIUM: good balance memory/speed\n",
    "    learning_rate = 0.0001             # Learning rate - MEDIUM: conservative for stability\n",
    "    patience = 7                       # Early stopping patience - MEDIUM: moderate patience\n",
    "    lradj = 'type1'                    # Learning rate adjustment strategy\n",
    "    \n",
    "    # Loss and optimization\n",
    "    loss = 'MSE'                       # Loss function: 'MSE', 'MAE', 'Huber'\n",
    "    use_amp = False                    # Automatic mixed precision (can speed up training)\n",
    "    \n",
    "    # System settings\n",
    "    num_workers = 6                    # DataLoader workers - MEDIUM: more parallel processing\n",
    "    seed = 2024                        # Random seed for reproducibility\n",
    "    \n",
    "    # Task specific\n",
    "    task_name = 'short_term_forecast'  # Task type: 'short_term_forecast' for financial prediction\n",
    "    \n",
    "    # Experiment tracking\n",
    "    des = 'medium_config'              # Experiment description\n",
    "    checkpoints = f'./checkpoints/TimesNet_medium_{datetime.now().strftime(\"%Y%m%d_%H%M\")}'\n",
    "    \n",
    "# Create config instance\n",
    "args = MediumConfig()\n",
    "\n",
    "print(\"‚öñÔ∏è Medium Configuration Loaded:\")\n",
    "print(f\"   üìè Sequence Length: {args.seq_len}\")\n",
    "print(f\"   üéØ Prediction Length: {args.pred_len}\")\n",
    "print(f\"   üß† Model Dimension: {args.d_model}\")\n",
    "print(f\"   ‚ö° Epochs: {args.train_epochs}\")\n",
    "print(f\"   üìä Batch Size: {args.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38423a4e",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Tweakable Parameters\n",
    "\n",
    "Modify these parameters to experiment with different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# TWEAKABLE PARAMETERS - EXPERIMENT\n",
    "# ================================\n",
    "\n",
    "# Modify these for experiments:\n",
    "\n",
    "# --- Sequence parameters (affect model complexity and data usage) ---\n",
    "args.seq_len = 100         # Try: 50, 100, 200 (longer = more context, slower training)\n",
    "args.pred_len = 10         # Try: 5, 10, 20 (longer = harder prediction task)\n",
    "args.label_len = 20        # Try: seq_len//5 to seq_len//2 (decoder start overlap)\n",
    "\n",
    "# --- Model size (affect memory usage and training time) ---\n",
    "args.d_model = 64          # Try: 32, 64, 128 (larger = more capacity, slower)\n",
    "args.d_ff = 128            # Try: 64, 128, 256 (usually 2x d_model)\n",
    "args.n_heads = 8           # Try: 4, 8, 16 (must divide d_model evenly)\n",
    "args.e_layers = 3          # Try: 2, 3, 4 (more layers = deeper model)\n",
    "\n",
    "# --- TimesNet specific (affect pattern recognition capability) ---\n",
    "args.top_k = 5             # Try: 3, 5, 8 (more frequencies = more complex patterns)\n",
    "args.num_kernels = 6       # Try: 4, 6, 8 (more kernels = more feature extraction)\n",
    "args.moving_avg = 25       # Try: 15, 25, 50 (window for trend decomposition)\n",
    "\n",
    "# --- Training parameters ---\n",
    "args.train_epochs = 20     # Try: 15, 20, 30\n",
    "args.batch_size = 32       # Try: 16, 32, 64 (larger = faster but more memory)\n",
    "args.learning_rate = 0.0001 # Try: 0.00005, 0.0001, 0.0005\n",
    "args.patience = 7          # Try: 5, 7, 10 (early stopping patience)\n",
    "\n",
    "# --- Advanced tweaks ---\n",
    "args.dropout = 0.1         # Try: 0.0, 0.1, 0.2 (higher = more regularization)\n",
    "args.factor = 1            # Try: 1, 3, 5 (attention sparsity factor)\n",
    "args.distil = True         # Try: True, False (knowledge distillation)\n",
    "args.activation = 'gelu'   # Try: 'gelu', 'relu', 'swish'\n",
    "\n",
    "# --- Loss function experiments ---\n",
    "args.loss = 'MSE'          # Try: 'MSE', 'MAE' (different loss characteristics)\n",
    "args.use_amp = False       # Try: True for faster training (if supported)\n",
    "\n",
    "print(f\"‚úèÔ∏è Updated Medium Configuration:\")\n",
    "print(f\"   Model Size: d_model={args.d_model}, d_ff={args.d_ff}, heads={args.n_heads}, layers={args.e_layers}\")\n",
    "print(f\"   TimesNet: top_k={args.top_k}, kernels={args.num_kernels}, moving_avg={args.moving_avg}\")\n",
    "print(f\"   Training: epochs={args.train_epochs}, batch={args.batch_size}, lr={args.learning_rate}\")\n",
    "print(f\"   Advanced: dropout={args.dropout}, factor={args.factor}, loss={args.loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10721277",
   "metadata": {},
   "source": [
    "## üöÄ Training Setup and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d423cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device and create checkpoint directory\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.makedirs(args.checkpoints, exist_ok=True)\n",
    "\n",
    "print(f\"üî• Using device: {device}\")\n",
    "print(f\"üìÅ Checkpoints: {args.checkpoints}\")\n",
    "\n",
    "# Data loader setup\n",
    "def create_data_loader(flag):\n",
    "    args.validation_length = args.val_len\n",
    "    args.test_length = args.test_len\n",
    "    \n",
    "    dataset = Dataset_Custom(\n",
    "        args=args,\n",
    "        root_path=args.root_path,\n",
    "        data_path=args.data_path,\n",
    "        flag=flag,\n",
    "        size=[args.seq_len, args.label_len, args.pred_len],\n",
    "        features=args.features,\n",
    "        target=args.target,\n",
    "        scale=True,\n",
    "        timeenc=1 if args.embed == 'timeF' else 0,\n",
    "        freq=args.freq\n",
    "    )\n",
    "    \n",
    "    shuffle = (flag == 'train')\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = create_data_loader('train')\n",
    "val_loader = create_data_loader('val')\n",
    "test_loader = create_data_loader('test')\n",
    "\n",
    "print(f\"üìä Data loaders created:\")\n",
    "print(f\"   Train: {len(train_loader)} batches\")\n",
    "print(f\"   Val: {len(val_loader)} batches\")\n",
    "print(f\"   Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00740299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TimesNet model\n",
    "model = TimesNet(args).to(device)\n",
    "\n",
    "# Setup training components\n",
    "if args.loss == 'MSE':\n",
    "    criterion = torch.nn.MSELoss()\n",
    "elif args.loss == 'MAE':\n",
    "    criterion = torch.nn.L1Loss()\n",
    "else:\n",
    "    criterion = torch.nn.MSELoss()  # Default fallback\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
    "\n",
    "# Model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"üß† TimesNet Medium Model Initialized:\")\n",
    "print(f\"   üìä Total Parameters: {total_params:,}\")\n",
    "print(f\"   üéØ Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"   üíæ Model Size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "print(f\"   üìà Loss Function: {args.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5759651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with detailed progress tracking\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"üèÉ Training on {num_batches} batches...\")\n",
    "    \n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        \n",
    "        # Prepare decoder input\n",
    "        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if args.use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                target_outputs = outputs[:, -args.pred_len:, :4]\n",
    "                target_y = batch_y[:, -args.pred_len:, :4]\n",
    "                loss = criterion(target_outputs, target_y)\n",
    "        else:\n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            target_outputs = outputs[:, -args.pred_len:, :4]\n",
    "            target_y = batch_y[:, -args.pred_len:, :4]\n",
    "            loss = criterion(target_outputs, target_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Progress reporting (every 15 batches for medium config)\n",
    "        if i % 15 == 0 or i == num_batches - 1:\n",
    "            progress_pct = (i + 1) / num_batches * 100\n",
    "            avg_loss = total_loss / (i + 1)\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            remaining = elapsed / (i + 1) * (num_batches - i - 1)\n",
    "            print(f\"   Batch {i+1:3d}/{num_batches} ({progress_pct:5.1f}%) - \"\n",
    "                  f\"Loss: {loss.item():.6f} (Avg: {avg_loss:.6f}) - \"\n",
    "                  f\"Remaining: {remaining:.1f}s\")\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"‚úÖ Epoch completed in {epoch_time:.1f}s. Average loss: {avg_loss:.6f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in val_loader:\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "            \n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "            \n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            \n",
    "            target_outputs = outputs[:, -args.pred_len:, :4]\n",
    "            target_y = batch_y[:, -args.pred_len:, :4]\n",
    "            loss = criterion(target_outputs, target_y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "print(\"üîß Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(f\"üöÄ Starting TimesNet Medium Training ({args.train_epochs} epochs)\")\n",
    "print(f\"‚è∞ Estimated time: ~{args.train_epochs * 20} minutes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(args.train_epochs):\n",
    "    print(f\"\\nüîÑ Epoch {epoch+1}/{args.train_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    print(\"üîç Running validation...\")\n",
    "    val_loss = validate_epoch()\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Log progress with trend analysis\n",
    "    trend_emoji = \"üìà\" if len(train_losses) > 1 and train_loss > train_losses[-2] else \"üìâ\"\n",
    "    print(f\"{trend_emoji} Epoch {epoch+1} Results: Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    # Learning rate adjustment\n",
    "    adjust_learning_rate(optimizer, epoch + 1, args)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"üéõÔ∏è Learning rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping(val_loss, model, args.checkpoints)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"‚èπÔ∏è Early stopping triggered\")\n",
    "        break\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f\"{args.checkpoints}/best_model.pth\")\n",
    "        print(f\"üíæ New best model saved (Val Loss: {val_loss:.6f})\")\n",
    "    \n",
    "    # Progress summary every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        elapsed_time = time.time() - training_start_time\n",
    "        remaining_epochs = args.train_epochs - (epoch + 1)\n",
    "        estimated_remaining = elapsed_time / (epoch + 1) * remaining_epochs\n",
    "        print(f\"‚è±Ô∏è Progress: {epoch+1}/{args.train_epochs} epochs ({elapsed_time/60:.1f}m elapsed, ~{estimated_remaining/60:.1f}m remaining)\")\n",
    "\n",
    "total_training_time = time.time() - training_start_time\n",
    "print(f\"\\nüéâ Training completed in {total_training_time/60:.1f} minutes!\")\n",
    "print(f\"üèÜ Best validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6525845b",
   "metadata": {},
   "source": [
    "## üìä Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and test\n",
    "model.load_state_dict(torch.load(f\"{args.checkpoints}/best_model.pth\", weights_only=False))\n",
    "model.eval()\n",
    "\n",
    "# Test evaluation\n",
    "preds = []\n",
    "trues = []\n",
    "\n",
    "print(\"üß™ Testing model...\")\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y, batch_x_mark, batch_y_mark in test_loader:\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        \n",
    "        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "        \n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        \n",
    "        pred = outputs[:, -args.pred_len:, :4].detach().cpu().numpy()\n",
    "        true = batch_y[:, -args.pred_len:, :4].detach().cpu().numpy()\n",
    "        \n",
    "        preds.append(pred)\n",
    "        trues.append(true)\n",
    "\n",
    "# Calculate metrics\n",
    "if preds:\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    \n",
    "    mae, mse, rmse, mape, mspe = metric(preds, trues)\n",
    "    \n",
    "    print(\"\\nüìä TimesNet Medium - Test Results:\")\n",
    "    print(f\"   üéØ MSE:  {mse:.6f}\")\n",
    "    print(f\"   üìè MAE:  {mae:.6f}\")\n",
    "    print(f\"   üìê RMSE: {rmse:.6f}\")\n",
    "    print(f\"   üìà MAPE: {mape:.6f}%\")\n",
    "    print(f\"   üìâ MSPE: {mspe:.6f}%\")\n",
    "    \n",
    "    # Training curve analysis\n",
    "    if len(train_losses) > 1:\n",
    "        final_improvement = (train_losses[0] - train_losses[-1]) / train_losses[0] * 100\n",
    "        val_improvement = (val_losses[0] - val_losses[-1]) / val_losses[0] * 100\n",
    "        print(f\"\\nüìà Training Analysis:\")\n",
    "        print(f\"   üìä Training improvement: {final_improvement:.1f}%\")\n",
    "        print(f\"   üìä Validation improvement: {val_improvement:.1f}%\")\n",
    "        print(f\"   üéØ Final train/val ratio: {train_losses[-1]/val_losses[-1]:.3f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nüìã Configuration Summary:\")\n",
    "    print(f\"   ‚öñÔ∏è Model: Medium ({total_params:,} params)\")\n",
    "    print(f\"   üìè Sequence: {args.seq_len} ‚Üí {args.pred_len}\")\n",
    "    print(f\"   üß† Architecture: d_model={args.d_model}, layers={args.e_layers}, heads={args.n_heads}\")\n",
    "    print(f\"   üîß TimesNet: top_k={args.top_k}, kernels={args.num_kernels}\")\n",
    "    print(f\"   ‚è±Ô∏è Training time: {total_training_time/60:.1f} minutes\")\n",
    "    print(f\"   üèÜ Final performance: RMSE={rmse:.6f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No test data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb45972",
   "metadata": {},
   "source": [
    "## üîç Model Insights (Optional Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02518ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Analyze model attention (if output_attention=True)\n",
    "if args.output_attention:\n",
    "    print(\"üîç Analyzing attention patterns...\")\n",
    "    \n",
    "    # Get a sample batch for attention analysis\n",
    "    sample_batch = next(iter(test_loader))\n",
    "    batch_x, batch_y, batch_x_mark, batch_y_mark = sample_batch\n",
    "    \n",
    "    batch_x = batch_x.float().to(device)\n",
    "    batch_y = batch_y.float().to(device)\n",
    "    batch_x_mark = batch_x_mark.float().to(device)\n",
    "    batch_y_mark = batch_y_mark.float().to(device)\n",
    "    \n",
    "    dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "    dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs, attentions = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        print(f\"üìä Attention shape: {attentions[0].shape if attentions else 'N/A'}\")\n",
    "else:\n",
    "    print(\"üí° Tip: Set args.output_attention=True for attention analysis\")\n",
    "\n",
    "# Performance per target analysis\n",
    "if preds.size > 0:\n",
    "    target_names = ['log_Open', 'log_High', 'log_Low', 'log_Close']\n",
    "    print(\"\\nüéØ Per-target Performance:\")\n",
    "    \n",
    "    for i, target in enumerate(target_names):\n",
    "        target_preds = preds[:, :, i].flatten()\n",
    "        target_trues = trues[:, :, i].flatten()\n",
    "        \n",
    "        target_mse = np.mean((target_preds - target_trues) ** 2)\n",
    "        target_mae = np.mean(np.abs(target_preds - target_trues))\n",
    "        \n",
    "        print(f\"   {target:10s}: MSE={target_mse:.6f}, MAE={target_mae:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Medium configuration analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
