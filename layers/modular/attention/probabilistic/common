import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class BayesianLinear(nn.Module):
    """
    A Bayesian linear layer that maintains distributions over its weights and biases.
    During training, it samples from these distributions. During inference, it uses
    the mean values. It also provides a method to calculate the KL divergence,
    which is used as a regularization term in the loss function.
    """
    def __init__(self, in_features: int, out_features: int, prior_std: float = 1.0):
        super().__init__()
        self.prior_std = prior_std
        
        # Variational posterior parameters (mean and log variance)
        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))
        self.weight_log_var = nn.Parameter(torch.Tensor(out_features, in_features))
        self.bias_mu = nn.Parameter(torch.Tensor(out_features))
        self.bias_log_var = nn.Parameter(torch.Tensor(out_features))
        
        self._initialize_parameters()

    def _initialize_parameters(self):
        """Initialize parameters with reasonable values."""
        nn.init.xavier_uniform_(self.weight_mu)
        nn.init.constant_(self.weight_log_var, -5.0)
        nn.init.zeros_(self.bias_mu)
        nn.init.constant_(self.bias_log_var, -5.0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.training:
            # Reparameterization trick: sample weights and biases
            weight_std = torch.exp(0.5 * self.weight_log_var)
            bias_std = torch.exp(0.5 * self.bias_log_var)
            
            weight = self.weight_mu + weight_std * torch.randn_like(self.weight_mu)
            bias = self.bias_mu + bias_std * torch.randn_like(self.bias_mu)
        else:
            # Use the mean for inference
            weight = self.weight_mu
            bias = self.bias_mu
            
        return F.linear(x, weight, bias)

    def kl_divergence(self) -> torch.Tensor:
        """
        Calculates the KL divergence between the variational posterior and the prior.
        """
        # Prior is N(0, prior_std^2)
        prior_log_var = 2 * math.log(self.prior_std)

        # KL for weights
        weight_var = torch.exp(self.weight_log_var)
        kl_weight = 0.5 * (prior_log_var - self.weight_log_var + (weight_var + self.weight_mu**2) / (self.prior_std**2) - 1).sum()
        
        # KL for biases
        bias_var = torch.exp(self.bias_log_var)
        kl_bias = 0.5 * (prior_log_var - self.bias_log_var + (bias_var + self.bias_mu**2) / (self.prior_std**2) - 1).sum()
        
        return kl_weight + kl_bias