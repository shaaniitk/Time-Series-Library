    
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple

from ..base import BaseAttention

class GraphAttentionLayer(BaseAttention):
    """
    An implementation of the Graph Attention Network (GAT) layer. It allows
    nodes to attend over their neighborhoods' features.
    """
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1, alpha: float = 0.2, **kwargs):
        super().__init__(d_model, n_heads)
        
        self.d_k = d_model // n_heads
        self.alpha = alpha # LeakyReLU negative slope

        self.W = nn.Linear(d_model, d_model, bias=False)
        self.a = nn.Parameter(torch.empty(size=(1, n_heads, 2 * self.d_k)))
        nn.init.xavier_uniform_(self.a.data, gain=1.414)
        
        self.dropout = nn.Dropout(dropout)
        self.leakyrelu = nn.LeakyReLU(self.alpha)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(
        self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None, # Here, attn_mask is the adjacency matrix
        **kwargs
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        
        B, N, _ = queries.shape # B: batch, N: number of nodes
        residual = queries
        
        # Linearly transform node features
        h = self.W(queries).view(B, N, self.n_heads, self.d_k)
        
        # Prepare for attention score computation
        h_i = h.permute(0, 2, 1, 3).unsqueeze(3) # [B, H, N, 1, D_k]
        h_j = h.permute(0, 2, 1, 3).unsqueeze(2) # [B, H, 1, N, D_k]
        
        # Concatenate features of pairs of nodes
        edge_h = torch.cat([h_i.expand(-1, -1, N, N, -1), h_j.expand(-1, -1, N, N, -1)], dim=-1)
        
        # Compute attention scores (un-normalized)
        e = self.leakyrelu((edge_h * self.a.unsqueeze(2).unsqueeze(3)).sum(dim=-1))
        
        # Masking with adjacency matrix
        if attn_mask is not None:
            # attn_mask is the adjacency matrix [B, N, N]
            e = e.masked_fill(attn_mask.unsqueeze(1) == 0, -1e9)
        
        attention = F.softmax(e, dim=-1)
        attention = self.dropout(attention)
        
        # Apply attention to node features
        context = torch.einsum('bhij,bhjd->bihd', attention, h.permute(0, 2, 1, 3))
        context = context.permute(0, 2, 1, 3).contiguous().view(B, N, self.d_model)
        
        output = self.layer_norm(self.dropout(context) + residual)

        return output, attention.mean(dim=1) # Return avg attention over heads

  