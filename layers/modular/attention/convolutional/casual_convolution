import torch
import torch.nn as nn
from typing import Optional, Tuple, List

from ..base import BaseAttention

class CausalConvolution(BaseAttention):
    """
    A module that uses multi-scale causal convolutions to process sequences.
    Causality is enforced by using appropriate padding and trimming.
    """
    def init(self, d_model: int, n_heads: int, kernel_sizes: List[int] = [3, 5, 7], dropout: float = 0.1, **kwargs):
        super().init(d_model, n_heads)
        self.conv_layers = nn.ModuleList()
        for k in kernel_sizes:
            # Padding ensures causality: (k-1) on the left, 0 on the right.
            padding = (k - 1, 0)
            conv = nn.Conv1d(d_model, d_model, kernel_size=k, padding=padding)
            self.conv_layers.append(conv)
            
        self.fusion_proj = nn.Linear(d_model * len(kernel_sizes), d_model)
        self.activation = nn.GELU()
        self.layer_norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(
        self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None,
        **kwargs
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        
        residual = queries
        x_conv = queries.transpose(1, 2) # [B, D, L]
        
        conv_outputs = []
        for conv in self.conv_layers:
            # The convolution output will have the same length due to padding
            out = self.activation(conv(x_conv))
            conv_outputs.append(out.transpose(1, 2)) # [B, L, D]

        # Concatenate features from different kernel sizes and fuse
        concatenated = torch.cat(conv_outputs, dim=-1)
        fused_output = self.fusion_proj(concatenated)
        
        output = self.layer_norm(self.dropout(fused_output) + residual)
        
        return output, None
    