# Training Script Workflow Analysis: train_celestial_production.py

## üìã **Executive Summary**

This document provides a comprehensive analysis of the production training script workflow for the Celestial Enhanced PGAT model, detailing every component, function, and file dependency used during training.

## üèóÔ∏è **Script Architecture Overview**

### **Main Entry Point**
- **File**: `scripts/train/train_celestial_production.py`
- **Purpose**: Heavy-duty overnight production training
- **Configuration**: `configs/celestial_enhanced_pgat_production.yaml`
- **Target**: OHLC prediction with seq_len=250

### **Key Design Principles**
1. **Production Stability**: Robust error handling and recovery
2. **Memory Efficiency**: Comprehensive memory monitoring and optimization
3. **Reproducibility**: Deterministic training with seed control
4. **Scalability**: Support for long sequences (250 timesteps)
5. **Monitoring**: Detailed logging and diagnostics

---

## üîÑ **Complete Workflow Sequence**

### **Phase 1: Initialization & Setup**

#### **1.1 Configuration Loading**
```python
def train_celestial_pgat_production():
    # Load YAML configuration
    config_path = "configs/celestial_enhanced_pgat_production.yaml"
    with open(config_path, 'r') as config_file:
        config_dict = yaml.safe_load(config_file)
    
    args = SimpleConfig(config_dict)
```

**Files Involved:**
- `configs/celestial_enhanced_pgat_production.yaml` - Production configuration
- `SimpleConfig` class - Dictionary-backed configuration with attribute access

#### **1.2 Logging Configuration**
```python
def configure_logging(config: SimpleConfig) -> logging.Logger:
    # Set up structured logging with file output
    # Configure memory diagnostics logger
    # Set appropriate log levels
```

**Features:**
- Structured logging with timestamps
- Separate memory diagnostics logger
- File-based log persistence
- Configurable log levels

#### **1.3 Device & Reproducibility Setup**
```python
def _ensure_reproducibility(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
```

**Components:**
- GPU/CPU device selection with fallback
- Reproducibility seed (2024) across all RNG sources
- CUDNN deterministic mode for consistent results

### **Phase 2: Data Module Preparation**

#### **2.1 Data Loading**
```python
def prepare_data_modules(args, logger, device) -> Dict[str, DataModule]:
    modules = {}
    for flag in ("train", "val", "test"):
        dataset, loader = data_provider(args, flag=flag)
        optimized_loader, diagnostics = _optimize_data_loader(...)
        modules[flag] = DataModule(dataset=dataset, loader=optimized_loader)
```

**Files Involved:**
- `data_provider/data_factory.py` - Data loading factory
- `data/prepared_financial_data.csv` - Input data (118 celestial features)

**Data Specifications:**
- **Training**: 6,750 samples, 421 batches, batch_size=16
- **Validation**: 41 samples, 3 batches
- **Test**: 41 samples, 3 batches
- **Features**: 118 celestial wave features ‚Üí 4 OHLC targets

#### **2.2 Data Loader Optimization**
```python
def _optimize_data_loader(dataset, loader, args, device, flag, logger):
    # Optimize batch size, shuffle, drop_last
    # Configure pin_memory for GPU efficiency
    # Set up persistent workers and prefetch
    # Add worker initialization for reproducibility
```

**Optimizations:**
- Pin memory for GPU transfers
- Persistent workers for efficiency
- Prefetch factor for pipeline optimization
- Worker seed initialization for reproducibility

#### **2.3 Scaling Utilities**
```python
# Extract scalers from training data
train_scaler = getattr(train_data, 'scaler', None)
target_scaler = getattr(train_data, 'target_scaler', None)
target_indices = [0, 1, 2, 3]  # OHLC indices
```

**Scaling Strategy:**
- Main scaler: 118 features (celestial waves)
- Target scaler: 4 features (OHLC)
- Separate scaling for inputs and targets

### **Phase 3: Model Initialization**

#### **3.1 Model Creation**
```python
model = Model(args).to(device)
```

**Files Involved:**
- `models/Celestial_Enhanced_PGAT.py` - Main model class

**Model Specifications:**
- **Parameters**: 12,310,208 total (12.3M)
- **Size**: ~47MB
- **Architecture**: Enhanced PGAT with celestial system
- **d_model**: Auto-adjusted 130 ‚Üí 208 for compatibility

#### **3.2 Component Activation Status**

**‚úÖ ACTIVE COMPONENTS:**
- `PhaseAwareCelestialProcessor` - Rich celestial feature extraction
- `CelestialBodyNodes` - 13 celestial body representations  
- `CelestialGraphCombinerFixed` - Memory-optimized batch processing
- `AdjacencyAwareGraphAttention` - Graph attention with adjacency masking
- `DynamicJointSpatioTemporalEncoding` - Time-varying spatiotemporal encoding
- `DecoderLayer` - Cross-attention decoder (2 layers)
- `DataEmbedding` - Token + Positional + Temporal embeddings

**‚ùå DISABLED COMPONENTS:**
- `SequentialMixtureDensityDecoder` - Disabled for stability
- `StochasticGraphLearner` - Disabled for production
- `HierarchicalTemporalSpatialMapper` - Disabled for simplicity

### **Phase 4: Training Setup**

#### **4.1 Optimizer Configuration**
```python
learning_rate = float(args.learning_rate)  # 0.001
weight_decay = float(getattr(args, 'weight_decay', 0.0001))
optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
```

#### **4.2 Mixed Precision Setup**
```python
use_amp = getattr(args, 'mixed_precision', False) and device.type == 'cuda'
if use_amp:
    scaler = GradScaler()
```

#### **4.3 Loss Function Selection**
```python
# Automatic loss selection based on model configuration
if getattr(model, 'use_mixture_decoder', False):
    criterion = SequentialMixtureNLLLoss(reduction='mean')
else:
    criterion = nn.MSELoss()  # Used in production
```

#### **4.4 Learning Rate Scheduling**
```python
def get_warmup_cosine_lr(epoch, warmup_epochs, total_epochs, base_lr, min_lr):
    if epoch < warmup_epochs:
        return base_lr * (epoch + 1) / warmup_epochs  # Linear warmup
    else:
        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
        return min_lr + (base_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))
```

**Schedule Parameters:**
- Warmup epochs: 5
- Base LR: 0.001
- Min LR: 1e-6
- Total epochs: 50

### **Phase 5: Training Loop Execution**

#### **5.1 Epoch Training**
```python
def train_epoch(model, train_loader, optimizer, criterion, scaler, use_amp, 
                gradient_accumulation_steps, device, epoch, args, logger, 
                target_scaler, target_indices, ...):
```

**Key Features:**
- **Gradient Accumulation**: 2 steps (effective batch size 32)
- **Mixed Precision**: AMP for GPU acceleration
- **Memory Monitoring**: Every 25 batches
- **Gradient Clipping**: `clip_grad_norm: 1.0`
- **Loss Computation**: MSE loss with proper target scaling

#### **5.2 Forward Pass Sequence**
```python
# 1. Data preparation
batch_x = batch_x.float().to(device)  # [16, 250, 118]
batch_y = batch_y.float().to(device)  # [16, 135, 118] (label_len + pred_len)

# 2. Decoder input preparation  
dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :])
dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1)

# 3. Model forward pass
outputs_raw = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)

# 4. Output normalization
outputs_tensor, aux_loss, mdn_outputs, metadata = _normalize_model_output(outputs_raw)

# 5. Target scaling for loss
y_true_for_loss = scale_targets_for_loss(batch_y[:, -args.pred_len:, :], 
                                        target_scaler, target_indices, device)

# 6. Loss computation
loss = criterion(outputs_tensor[:, -args.pred_len:, :4], y_true_for_loss)
```

#### **5.3 Model Forward Pass Breakdown**

**Input Processing:**
```python
# Phase-aware celestial processing
celestial_features, adjacency_matrix, phase_metadata = self.phase_aware_processor(x_enc)
# [16, 250, 118] ‚Üí [16, 250, 416] (13√ó32D celestial features)

# Data embedding
enc_out = self.enc_embedding(celestial_features, x_mark_enc)
# [16, 250, 416] ‚Üí [16, 250, 208]
```

**Celestial Graph Processing:**
```python
# Generate celestial adjacency matrices
astronomical_adj, dynamic_adj, celestial_features, metadata = self.celestial_nodes(enc_out)

# Learn data-driven adjacency
learned_adj = self._learn_data_driven_graph(enc_out)

# Combine all adjacency matrices (FIXED VERSION - BATCH PROCESSING)
combined_adj, fusion_metadata = self.celestial_combiner(
    astronomical_adj, learned_adj, dynamic_adj, enc_out
)
```

**Spatiotemporal Encoding:**
```python
# Dynamic spatiotemporal encoding with time-varying adjacencies
encoded_features = self.dynamic_spatiotemporal_encoder(enc_out, combined_adj)
```

**Graph Attention Processing:**
```python
# Efficient covariate interaction (partitioned processing)
if self.use_efficient_covariate_interaction:
    graph_features = self._efficient_graph_processing(encoded_features, combined_adj)
else:
    # Traditional graph attention layers
    for layer in self.graph_attention_layers:
        graph_features = layer(graph_features, combined_adj)
```

**Decoder Processing:**
```python
# Cross-attention decoder layers
for layer in self.decoder_layers:
    decoder_features = layer(decoder_features, graph_features)

# Final projection to OHLC
predictions = self.projection(decoder_features[:, -self.pred_len:, :])
```

#### **5.4 Validation Epoch**
```python
def validate_epoch(model, val_loader, criterion, device, epoch, args, logger,
                  target_scaler, target_indices, ...):
    model.eval()
    with torch.no_grad():
        # Same forward pass as training but without gradients
        # Memory monitoring and loss computation
```

#### **5.5 Learning Rate Adjustment**
```python
def adjust_learning_rate_warmup_cosine(optimizer, epoch, args):
    lr = get_warmup_cosine_lr(epoch, warmup_epochs, total_epochs, base_lr, min_lr)
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    return lr
```

### **Phase 6: Checkpoint Management**

#### **6.1 Checkpoint Saving**
```python
def manage_checkpoints(artifacts, model, optimizer, epoch, train_loss, val_loss,
                      current_lr, checkpoint_dir, save_best_only, checkpoint_interval,
                      logger, is_best):
    checkpoint_payload = {
        "epoch": epoch + 1,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "val_loss": val_loss,
        "train_loss": train_loss,
        "lr": current_lr,
    }
```

**Checkpoint Strategy:**
- Best model saved on validation improvement
- Regular checkpoints every 5 epochs
- Complete state preservation (model + optimizer + metrics)

### **Phase 7: Model Evaluation**

#### **7.1 Baseline Evaluation**
```python
def evaluate_model(model, test_loader, test_data, device, args, logger, target_indices):
    # Collect predictions on test set
    preds, trues, processed = collect_predictions(...)
    
    # Compute metrics
    mae, mse, rmse, mape, mspe = metric(preds, trues)
    
    # Per-target analysis (OHLC)
    for index, name in enumerate(['Open', 'High', 'Low', 'Close']):
        pred_i = preds[:, :, index:index + 1]
        true_i = trues[:, :, index:index + 1]
        mae_i, mse_i, rmse_i, mape_i, mspe_i = metric(pred_i, true_i)
```

#### **7.2 Adversarial Diagnostics**
```python
def run_adversarial_diagnostics(model, test_loader, device, args, logger,
                               target_indices, num_samples):
    scenarios = [
        ("gaussian_noise", lambda batch: batch + torch.randn_like(batch) * 0.05),
        ("zero_inputs", lambda batch: torch.zeros_like(batch)),
        ("scale_spike", lambda batch: batch * 1.25),
        ("time_reverse", lambda batch: torch.flip(batch, dims=[1])),
        ("feature_dropout", lambda batch: batch * (torch.rand_like(batch) > 0.1).float()),
    ]
```

**Robustness Testing:**
- Gaussian noise stress test
- Zero input handling
- Scale spike robustness  
- Time reversal test
- Feature dropout simulation

### **Phase 8: Results Persistence**

#### **8.1 Results Compilation**
```python
def persist_training_results(checkpoint_dir, config_dict, args, total_params,
                           artifacts, evaluation_result, adversarial_results,
                           training_duration_seconds, rmse_value, ...):
    results = {
        "model": "Celestial_Enhanced_PGAT_PRODUCTION",
        "task": "OHLC_Prediction_Production", 
        "config": "HEAVY_DUTY_OVERNIGHT",
        "num_targets": 4,
        "seq_len": 250,
        "pred_len": 10,
        "total_parameters": 12310208,
        "training_time_hours": training_duration_hours,
        "overall_metrics": overall_metrics,
        "per_target_metrics": per_target_metrics,
        "adversarial_diagnostics": adversarial_results,
        "train_losses": artifacts.train_losses,
        "val_losses": artifacts.val_losses,
        "best_val_loss": artifacts.best_val_loss,
        "config_dict": config_dict,
        "timestamp": datetime.now().isoformat(),
    }
```

---

## üß† **Memory Management System**

### **Memory Monitoring**
```python
def _gather_memory_stats(device: torch.device) -> Dict[str, Union[float, str]]:
    # CPU memory via psutil
    # GPU memory via torch.cuda
    # Process-specific memory tracking
    # Memory allocation and reservation stats
```

### **Memory Logging**
```python
def _log_memory_snapshot(stage: str, device: torch.device, logger, context=None):
    # Structured JSON logging
    # Stage-specific memory tracking
    # Context information inclusion
    # Exception-safe logging
```

### **Memory Optimization Features**
- **Gradient Checkpointing**: Enabled in celestial combiner
- **Mixed Precision**: AMP for GPU memory efficiency
- **Efficient Covariate Interaction**: Partitioned graph processing
- **Memory Cleanup**: Explicit cleanup on errors
- **Memory Limits**: Configurable memory usage limits

---

## üìä **Performance Characteristics**

### **Training Performance**
- **Epoch Time**: ~475 seconds per epoch
- **Memory Usage**: 2-6GB (vs. >64GB with buggy combiner)
- **Batch Processing**: 16 samples per batch, effective 32 with accumulation
- **Sequence Length**: 250 timesteps (long sequence support)

### **Model Performance**
- **Parameters**: 12.3M trainable parameters
- **Memory Footprint**: ~47MB model size
- **Forward Pass**: ~2.5GB memory usage
- **Inference Speed**: Real-time capable

### **Optimization Results**
- **Memory Reduction**: 70-80% vs. original implementation
- **Training Stability**: No OOM errors with seq_len=250
- **Convergence**: Stable training with proper loss computation
- **Robustness**: Passes 5 adversarial test scenarios

---

## üîß **Key Utility Functions**

### **Target Scaling**
```python
def scale_targets_for_loss(targets_unscaled, target_scaler, target_indices, device):
    # Extract OHLC targets
    # Apply target scaler transformation
    # Reshape for loss computation
    # Return scaled targets tensor
```

### **Output Normalization**
```python
def _normalize_model_output(raw_output):
    # Handle tuple/list outputs
    # Extract auxiliary loss
    # Identify MDN components
    # Extract metadata
    # Return (tensor, aux_loss, mdn_tuple, metadata)
```

### **Learning Rate Scheduling**
```python
def get_warmup_cosine_lr(epoch, warmup_epochs, total_epochs, base_lr, min_lr):
    # Linear warmup phase
    # Cosine annealing phase
    # Smooth transition between phases
```

---

## üìÅ **Complete File Dependencies**

### **Core Training Files**
- `scripts/train/train_celestial_production.py` - Main training script
- `configs/celestial_enhanced_pgat_production.yaml` - Production configuration

### **Model Files**
- `models/Celestial_Enhanced_PGAT.py` - Main model implementation
- `layers/modular/graph/celestial_graph_combiner_fixed.py` - Fixed memory-efficient combiner
- `layers/modular/aggregation/phase_aware_celestial_processor.py` - Phase-aware processing
- `layers/modular/graph/adjacency_aware_attention.py` - Graph attention layers
- `layers/modular/encoder/spatiotemporal_encoding.py` - Spatiotemporal encoders
- `layers/modular/graph/celestial_body_nodes.py` - Celestial body representations

### **Utility Files**
- `data_provider/data_factory.py` - Data loading factory
- `utils/tools.py` - Training utilities (EarlyStopping, adjust_learning_rate)
- `utils/metrics.py` - Evaluation metrics
- `utils/celestial_wave_aggregator.py` - Wave aggregation utilities

### **Data Files**
- `data/prepared_financial_data.csv` - Input dataset (118 celestial features)

---

## üéØ **Success Metrics**

### **Training Success Indicators**
- ‚úÖ Model initializes without errors
- ‚úÖ Memory usage stays under 6GB
- ‚úÖ Training completes 50 epochs
- ‚úÖ Validation loss decreases over time
- ‚úÖ No OOM errors with seq_len=250
- ‚úÖ Checkpoints save successfully

### **Model Performance Indicators**
- ‚úÖ RMSE < 1.0 on test set
- ‚úÖ All OHLC targets predicted
- ‚úÖ Adversarial tests pass
- ‚úÖ Memory efficiency maintained
- ‚úÖ Training stability achieved

---

## üöÄ **Conclusion**

The `train_celestial_production.py` script represents a sophisticated, production-ready training system that successfully handles the complex Celestial Enhanced PGAT model with seq_len=250. Key achievements include:

1. **Memory Efficiency**: 70-80% memory reduction through fixed components
2. **Production Stability**: Robust error handling and recovery mechanisms  
3. **Comprehensive Monitoring**: Detailed memory and performance diagnostics
4. **Scalable Architecture**: Support for long sequences and large models
5. **Reproducible Results**: Deterministic training with seed control

The workflow successfully combines advanced astrological AI modeling with production-grade engineering practices, enabling reliable overnight training runs for financial time series forecasting.