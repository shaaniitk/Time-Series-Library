CELESTIAL ENHANCED PGAT - TRAINING DIAGNOSTICS
================================================================================
Config: configs/celestial_production_deep_ultimate_fixed.yaml
Start time: 2025-11-01 11:15:33
================================================================================


--- MODEL OUTPUT DIAGNOSTICS ---
outputs_tensor.shape: (2, 20, 4)
outputs_tensor.mean/std: -0.113964 / 0.441298
y_true_for_loss.shape: (2, 20, 4)
y_true_for_loss.mean/std: -0.093669 / 0.700912
c_out_evaluation: 4
enable_mdn flag: True
mdn.pi.shape: (2, 20, 4, 8)
mdn.mu.shape: (2, 20, 4, 8)
mdn.sigma.shape: (2, 20, 4, 8)
mdn.mixture_mean.shape: (2, 20, 4)
mdn.mixture_mean.mean/std: -0.212047 / 0.346889
outputs_tensor sample (first up to 8 values): [0.38990530371665955, -0.17834538221359253, -0.6293145418167114, -0.7770043611526489, 0.020000673830509186, 0.149880513548851, -0.5162447094917297, -0.9515455961227417]

================================================================================
EPOCH 1 | BATCH 0/3195 | TRAINING MODE
================================================================================
raw_loss (full batch loss): 1.65153146
loss (scaled for backward): 0.55051047
effective_cycle (gradient_accumulation_steps): 3
loss/raw_loss ratio: 0.3333 (should be ~1/3)
accumulated train_loss so far: 1.65153146
avg train_loss so far: 1.65153146
NOTE: NOW ACCUMULATING 'loss' (scaled), NOT 'raw_loss' (3x inflated)

--- MODEL OUTPUT DIAGNOSTICS ---
outputs_tensor.shape: (2, 20, 4)
outputs_tensor.mean/std: -0.119101 / 0.417259
y_true_for_loss.shape: (2, 20, 4)
y_true_for_loss.mean/std: 0.086403 / 0.463772
c_out_evaluation: 4
enable_mdn flag: True
mdn.pi.shape: (2, 20, 4, 8)
mdn.mu.shape: (2, 20, 4, 8)
mdn.sigma.shape: (2, 20, 4, 8)
mdn.mixture_mean.shape: (2, 20, 4)
mdn.mixture_mean.mean/std: -0.117420 / 0.301451
outputs_tensor sample (first up to 8 values): [-0.49380114674568176, 0.5394832491874695, 0.03447984904050827, 0.7110903263092041, -0.02833489328622818, 0.1359371542930603, 0.9328359961509705, 0.08909637480974197]

================================================================================
EPOCH 1 | BATCH 1/3195 | TRAINING MODE
================================================================================
raw_loss (full batch loss): 1.53779280
loss (scaled for backward): 0.51259762
effective_cycle (gradient_accumulation_steps): 3
loss/raw_loss ratio: 0.3333 (should be ~1/3)
accumulated train_loss so far: 3.18932426
avg train_loss so far: 1.59466213
NOTE: NOW ACCUMULATING 'loss' (scaled), NOT 'raw_loss' (3x inflated)

--- MODEL OUTPUT DIAGNOSTICS ---
outputs_tensor.shape: (2, 20, 4)
outputs_tensor.mean/std: -0.352339 / 0.514239
y_true_for_loss.shape: (2, 20, 4)
y_true_for_loss.mean/std: -0.021968 / 0.802170
c_out_evaluation: 4
enable_mdn flag: True
mdn.pi.shape: (2, 20, 4, 8)
mdn.mu.shape: (2, 20, 4, 8)
mdn.sigma.shape: (2, 20, 4, 8)
mdn.mixture_mean.shape: (2, 20, 4)
mdn.mixture_mean.mean/std: -0.270963 / 0.431681
outputs_tensor sample (first up to 8 values): [-0.5994528532028198, -0.17551353573799133, -0.29346540570259094, -0.6925519704818726, -0.644940972328186, -0.527404248714447, -0.2498035579919815, -0.008366953581571579]

--- OPTIMIZER STEP DIAGNOSTICS ---
embedding_module.celestial_projection.0.weight:
  weight_norm_before: 24.97585297
  weight_norm_after: 24.97034836
  weight_change: 0.00550461
  grad_norm: 0.06375099
embedding_module.celestial_projection.0.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027917
  weight_change: 0.00027917
  grad_norm: 0.00520879
embedding_module.celestial_projection.1.weight:
  weight_norm_before: 27.92848015
  weight_norm_after: 27.92819405
  weight_change: 0.00028610
  grad_norm: 0.00333963
embedding_module.celestial_projection.1.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027909
  weight_change: 0.00027909
  grad_norm: 0.00329120
embedding_module.calendar_effects_encoder.calendar_projection.0.weight:
  weight_norm_before: 12.84800053
  weight_norm_after: 12.84655952
  weight_change: 0.00144100
  grad_norm: 0.01867496
embedding_module.calendar_effects_encoder.calendar_projection.0.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00013958
  weight_change: 0.00013958
  grad_norm: 0.00715255
embedding_module.calendar_effects_encoder.calendar_projection.1.weight:
  weight_norm_before: 13.96424007
  weight_norm_after: 13.96410275
  weight_change: 0.00013733
  grad_norm: 0.00208492
embedding_module.calendar_effects_encoder.calendar_projection.1.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00013942
  weight_change: 0.00013942
  grad_norm: 0.00188251
graph_module.celestial_query_projection.weight:
  weight_norm_before: 12.06245327
  weight_norm_after: 12.06029034
  weight_change: 0.00216293
  grad_norm: 0.02086639
graph_module.celestial_query_projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00008890
  weight_change: 0.00008890
  grad_norm: 0.00093535
graph_module.celestial_key_projection.weight:
  weight_norm_before: 12.01940632
  weight_norm_after: 12.01724815
  weight_change: 0.00215816
  grad_norm: 0.01826090
graph_module.celestial_key_projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00000010
  weight_change: 0.00000010
  grad_norm: 0.00000000
graph_module.celestial_value_projection.weight:
  weight_norm_before: 12.00751686
  weight_norm_after: 12.00538731
  weight_change: 0.00212955
  grad_norm: 0.03542136
graph_module.celestial_value_projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00008943
  weight_change: 0.00008943
  grad_norm: 0.00517563
graph_module.celestial_output_projection.weight:
  weight_norm_before: 12.06493092
  weight_norm_after: 12.06279182
  weight_change: 0.00213909
  grad_norm: 0.03819545
graph_module.celestial_output_projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027924
  weight_change: 0.00027924
  grad_norm: 0.02062054
encoder_module.hierarchical_mapper.spatial_projection.weight:
  weight_norm_before: 27.91785812
  weight_norm_after: 27.91785812
  weight_change: 0.00000000
  grad_norm: 0.00000000
encoder_module.hierarchical_mapper.spatial_projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00000000
  weight_change: 0.00000000
  grad_norm: 0.00000000
encoder_module.hierarchical_projection.weight:
  weight_norm_before: 38.05081177
  weight_norm_after: 38.05081177
  weight_change: 0.00000000
  grad_norm: 0.00000000
encoder_module.hierarchical_projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00000000
  weight_change: 0.00000000
  grad_norm: 0.00000000
encoder_module.spatiotemporal_encoder.node_feature_projection.weight:
  weight_norm_before: 38.05606079
  weight_norm_after: 38.05606079
  weight_change: 0.00000000
  grad_norm: 0.00000000
encoder_module.spatiotemporal_encoder.node_feature_projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00000000
  weight_change: 0.00000000
  grad_norm: 0.00000000
encoder_module.spatiotemporal_encoder.node_feature_back_projection.weight:
  weight_norm_before: 38.05195999
  weight_norm_after: 38.05195999
  weight_change: 0.00000000
  grad_norm: 0.00000000
encoder_module.spatiotemporal_encoder.node_feature_back_projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00000000
  weight_change: 0.00000000
  grad_norm: 0.00000000
decoder_module.dual_stream_decoder.target_autocorr.target_projection.weight:
  weight_norm_before: 27.93434525
  weight_norm_after: 27.92774010
  weight_change: 0.00660515
  grad_norm: 0.11278842
decoder_module.dual_stream_decoder.target_autocorr.target_projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027914
  weight_change: 0.00027914
  grad_norm: 0.00478924
decoder_module.dual_stream_decoder.target_autocorr.output_projection.0.weight:
  weight_norm_before: 27.94274330
  weight_norm_after: 27.93611145
  weight_change: 0.00663185
  grad_norm: 0.10400676
decoder_module.dual_stream_decoder.target_autocorr.output_projection.0.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027928
  weight_change: 0.00027928
  grad_norm: 0.11832670
decoder_module.dual_stream_decoder.target_autocorr.output_projection.1.weight:
  weight_norm_before: 27.92848015
  weight_norm_after: 27.92819405
  weight_change: 0.00028610
  grad_norm: 0.00371963
decoder_module.dual_stream_decoder.target_autocorr.output_projection.1.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027901
  weight_change: 0.00027901
  grad_norm: 0.00382810
decoder_module.celestial_to_target_attention.target_query_projections.0.weight:
  weight_norm_before: 27.91936874
  weight_norm_after: 27.91259384
  weight_change: 0.00677490
  grad_norm: 0.00212439
decoder_module.celestial_to_target_attention.target_query_projections.0.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027638
  weight_change: 0.00027638
  grad_norm: 0.00011395
decoder_module.celestial_to_target_attention.target_query_projections.1.weight:
  weight_norm_before: 27.91475105
  weight_norm_after: 27.90799713
  weight_change: 0.00675392
  grad_norm: 0.00179916
decoder_module.celestial_to_target_attention.target_query_projections.1.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027626
  weight_change: 0.00027626
  grad_norm: 0.00009732
decoder_module.celestial_to_target_attention.target_query_projections.2.weight:
  weight_norm_before: 27.93503380
  weight_norm_after: 27.92823410
  weight_change: 0.00679970
  grad_norm: 0.00178209
decoder_module.celestial_to_target_attention.target_query_projections.2.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027603
  weight_change: 0.00027603
  grad_norm: 0.00009465
decoder_module.celestial_to_target_attention.target_query_projections.3.weight:
  weight_norm_before: 27.92885780
  weight_norm_after: 27.92207527
  weight_change: 0.00678253
  grad_norm: 0.00189678
decoder_module.celestial_to_target_attention.target_query_projections.3.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027639
  weight_change: 0.00027639
  grad_norm: 0.00010425
decoder_module.celestial_to_target_attention.celestial_key_projection.weight:
  weight_norm_before: 27.93284225
  weight_norm_after: 27.92608643
  weight_change: 0.00675583
  grad_norm: 0.00385975
decoder_module.celestial_to_target_attention.celestial_key_projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00000004
  weight_change: 0.00000004
  grad_norm: 0.00000000
decoder_module.celestial_to_target_attention.celestial_value_projection.weight:
  weight_norm_before: 27.91750145
  weight_norm_after: 27.91075897
  weight_change: 0.00674248
  grad_norm: 0.02912300
decoder_module.celestial_to_target_attention.celestial_value_projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027898
  weight_change: 0.00027898
  grad_norm: 0.00270855
decoder_module.celestial_to_target_attention.output_projections.0.0.weight:
  weight_norm_before: 27.92615891
  weight_norm_after: 27.91944122
  weight_change: 0.00671768
  grad_norm: 0.03668087
decoder_module.celestial_to_target_attention.output_projections.0.0.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027836
  weight_change: 0.00027836
  grad_norm: 0.00129123
decoder_module.celestial_to_target_attention.output_projections.0.3.weight:
  weight_norm_before: 27.92459869
  weight_norm_after: 27.91784477
  weight_change: 0.00675392
  grad_norm: 0.03628684
decoder_module.celestial_to_target_attention.output_projections.0.3.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027911
  weight_change: 0.00027911
  grad_norm: 0.00193115
decoder_module.celestial_to_target_attention.output_projections.1.0.weight:
  weight_norm_before: 27.93393707
  weight_norm_after: 27.92720222
  weight_change: 0.00673485
  grad_norm: 0.03443698
decoder_module.celestial_to_target_attention.output_projections.1.0.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027844
  weight_change: 0.00027844
  grad_norm: 0.00121006
decoder_module.celestial_to_target_attention.output_projections.1.3.weight:
  weight_norm_before: 27.94378281
  weight_norm_after: 27.93703842
  weight_change: 0.00674438
  grad_norm: 0.03438934
decoder_module.celestial_to_target_attention.output_projections.1.3.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027911
  weight_change: 0.00027911
  grad_norm: 0.00193115
decoder_module.celestial_to_target_attention.output_projections.2.0.weight:
  weight_norm_before: 27.92011261
  weight_norm_after: 27.91336632
  weight_change: 0.00674629
  grad_norm: 0.03378796
decoder_module.celestial_to_target_attention.output_projections.2.0.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027850
  weight_change: 0.00027850
  grad_norm: 0.00114481
decoder_module.celestial_to_target_attention.output_projections.2.3.weight:
  weight_norm_before: 27.92302895
  weight_norm_after: 27.91629219
  weight_change: 0.00673676
  grad_norm: 0.03591944
decoder_module.celestial_to_target_attention.output_projections.2.3.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027911
  weight_change: 0.00027911
  grad_norm: 0.00193115
decoder_module.celestial_to_target_attention.output_projections.3.0.weight:
  weight_norm_before: 27.90814018
  weight_norm_after: 27.90138245
  weight_change: 0.00675774
  grad_norm: 0.03464963
decoder_module.celestial_to_target_attention.output_projections.3.0.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027834
  weight_change: 0.00027834
  grad_norm: 0.00118389
decoder_module.celestial_to_target_attention.output_projections.3.3.weight:
  weight_norm_before: 27.94851112
  weight_norm_after: 27.94175148
  weight_change: 0.00675964
  grad_norm: 0.03465931
decoder_module.celestial_to_target_attention.output_projections.3.3.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00027911
  weight_change: 0.00027911
  grad_norm: 0.00193115
decoder_module.projection.weight:
  weight_norm_before: 2.79453063
  weight_norm_after: 2.79453063
  weight_change: 0.00000000
  grad_norm: 0.00000000
decoder_module.projection.bias:
  weight_norm_before: 0.00000000
  weight_norm_after: 0.00000000
  weight_change: 0.00000000
  grad_norm: 0.00000000
decoder_module.celestial_projection.weight:
  weight_norm_before: 16.12640762
  weight_norm_after: 16.12640762
  weight_change: 0.00000000
  grad_norm: 0.02532240
decoder_module.celestial_projection.bias:
  weight_norm_before: 2.51884103
  weight_norm_after: 2.51884103
  weight_change: 0.00000000
  grad_norm: 0.00946803
optimizer_step_executed: True
current_lr: 0.00001000

================================================================================
EPOCH 1 | BATCH 2/3195 | TRAINING MODE
================================================================================
raw_loss (full batch loss): 1.63031542
loss (scaled for backward): 0.54343849
effective_cycle (gradient_accumulation_steps): 3
loss/raw_loss ratio: 0.3333 (should be ~1/3)
accumulated train_loss so far: 4.81963968
avg train_loss so far: 1.60654656
NOTE: NOW ACCUMULATING 'loss' (scaled), NOT 'raw_loss' (3x inflated)

--- MODEL OUTPUT DIAGNOSTICS ---
outputs_tensor.shape: (2, 20, 4)
outputs_tensor.mean/std: -0.246911 / 0.541797
y_true_for_loss.shape: (2, 20, 4)
y_true_for_loss.mean/std: -0.119716 / 0.687644
c_out_evaluation: 4
enable_mdn flag: True
mdn.pi.shape: (2, 20, 4, 8)
mdn.mu.shape: (2, 20, 4, 8)
mdn.sigma.shape: (2, 20, 4, 8)
mdn.mixture_mean.shape: (2, 20, 4)
mdn.mixture_mean.mean/std: -0.167459 / 0.367508
outputs_tensor sample (first up to 8 values): [-0.2866799235343933, 0.19241128861904144, 0.07469628006219864, -0.1271359920501709, 0.1785746067762375, 0.08711370080709457, 0.2825619578361511, -0.28102540969848633]

================================================================================
EPOCH 1 | BATCH 3/3195 | TRAINING MODE
================================================================================
raw_loss (full batch loss): 1.70348012
loss (scaled for backward): 0.56782669
effective_cycle (gradient_accumulation_steps): 3
loss/raw_loss ratio: 0.3333 (should be ~1/3)
accumulated train_loss so far: 6.52311981
avg train_loss so far: 1.63077995
NOTE: NOW ACCUMULATING 'loss' (scaled), NOT 'raw_loss' (3x inflated)

--- MODEL OUTPUT DIAGNOSTICS ---
outputs_tensor.shape: (2, 20, 4)
outputs_tensor.mean/std: -0.255696 / 0.412651
y_true_for_loss.shape: (2, 20, 4)
y_true_for_loss.mean/std: -0.340623 / 1.061846
c_out_evaluation: 4
enable_mdn flag: True
mdn.pi.shape: (2, 20, 4, 8)
mdn.mu.shape: (2, 20, 4, 8)
mdn.sigma.shape: (2, 20, 4, 8)
mdn.mixture_mean.shape: (2, 20, 4)
mdn.mixture_mean.mean/std: -0.189723 / 0.289510
outputs_tensor sample (first up to 8 values): [0.2627151608467102, -0.2764492630958557, -0.6558408737182617, -0.8198246359825134, -0.02723030000925064, -0.04696033522486687, -0.4507627785205841, -0.9556304216384888]

================================================================================
EPOCH 1 | BATCH 4/3195 | TRAINING MODE
================================================================================
raw_loss (full batch loss): 1.75397873
loss (scaled for backward): 0.58465958
effective_cycle (gradient_accumulation_steps): 3
loss/raw_loss ratio: 0.3333 (should be ~1/3)
accumulated train_loss so far: 8.27709854
avg train_loss so far: 1.65541971
NOTE: NOW ACCUMULATING 'loss' (scaled), NOT 'raw_loss' (3x inflated)

--- MODEL OUTPUT DIAGNOSTICS ---
outputs_tensor.shape: (2, 20, 4)
outputs_tensor.mean/std: -0.239651 / 0.545167
y_true_for_loss.shape: (2, 20, 4)
y_true_for_loss.mean/std: 0.030441 / 0.491041
c_out_evaluation: 4
enable_mdn flag: True
mdn.pi.shape: (2, 20, 4, 8)
mdn.mu.shape: (2, 20, 4, 8)
mdn.sigma.shape: (2, 20, 4, 8)
mdn.mixture_mean.shape: (2, 20, 4)
mdn.mixture_mean.mean/std: -0.260005 / 0.417366
outputs_tensor sample (first up to 8 values): [0.6536495685577393, -0.20665618777275085, 0.061059482395648956, -1.1712976694107056, 0.23518244922161102, 0.3584732115268707, 0.16852089762687683, -1.081619143486023]

--- MODEL OUTPUT DIAGNOSTICS ---
outputs_tensor.shape: (2, 20, 4)
outputs_tensor.mean/std: -0.250611 / 0.357937
y_true_for_loss.shape: (2, 20, 4)
y_true_for_loss.mean/std: -0.065337 / 1.023086
c_out_evaluation: 4
enable_mdn flag: True
mdn.pi.shape: (2, 20, 4, 8)
mdn.mu.shape: (2, 20, 4, 8)
mdn.sigma.shape: (2, 20, 4, 8)
mdn.mixture_mean.shape: (2, 20, 4)
mdn.mixture_mean.mean/std: -0.203554 / 0.331213
outputs_tensor sample (first up to 8 values): [-0.3793853521347046, -0.25382566452026367, 0.019575659185647964, -0.29311472177505493, -0.33334997296333313, -0.08619873225688934, 0.2971761226654053, -0.9740645885467529]

--- MODEL OUTPUT DIAGNOSTICS ---
outputs_tensor.shape: (2, 20, 4)
outputs_tensor.mean/std: -0.086517 / 0.341299
y_true_for_loss.shape: (2, 20, 4)
y_true_for_loss.mean/std: -0.094536 / 0.934213
c_out_evaluation: 4
enable_mdn flag: True
mdn.pi.shape: (2, 20, 4, 8)
mdn.mu.shape: (2, 20, 4, 8)
mdn.sigma.shape: (2, 20, 4, 8)
mdn.mixture_mean.shape: (2, 20, 4)
mdn.mixture_mean.mean/std: -0.071448 / 0.249825
outputs_tensor sample (first up to 8 values): [0.5048084259033203, -0.25600892305374146, 0.035188354551792145, -0.5455447435379028, 0.02893906831741333, 0.19797897338867188, 0.2356208711862564, -0.3418027460575104]

--- MODEL OUTPUT DIAGNOSTICS ---
outputs_tensor.shape: (2, 20, 4)
outputs_tensor.mean/std: -0.281111 / 0.423106
y_true_for_loss.shape: (2, 20, 4)
y_true_for_loss.mean/std: -0.075182 / 0.631042
c_out_evaluation: 4
enable_mdn flag: True
mdn.pi.shape: (2, 20, 4, 8)
mdn.mu.shape: (2, 20, 4, 8)
mdn.sigma.shape: (2, 20, 4, 8)
mdn.mixture_mean.shape: (2, 20, 4)
mdn.mixture_mean.mean/std: -0.229370 / 0.343429
outputs_tensor sample (first up to 8 values): [0.44443660974502563, -0.11355757713317871, 0.6469317674636841, -0.6870938539505005, -0.05094487592577934, -0.06272497773170471, -0.1754547357559204, -1.0135853290557861]

--- MODEL OUTPUT DIAGNOSTICS ---
outputs_tensor.shape: (2, 20, 4)
outputs_tensor.mean/std: -0.302271 / 0.417347
y_true_for_loss.shape: (2, 20, 4)
y_true_for_loss.mean/std: 0.013353 / 0.562121
c_out_evaluation: 4
enable_mdn flag: True
mdn.pi.shape: (2, 20, 4, 8)
mdn.mu.shape: (2, 20, 4, 8)
mdn.sigma.shape: (2, 20, 4, 8)
mdn.mixture_mean.shape: (2, 20, 4)
mdn.mixture_mean.mean/std: -0.212766 / 0.379888
outputs_tensor sample (first up to 8 values): [0.17803609371185303, -0.10990747809410095, 0.28526240587234497, -0.981764554977417, 0.13723674416542053, -0.05024942383170128, -0.566218376159668, -0.664322018623352]
