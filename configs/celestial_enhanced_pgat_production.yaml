# Celestial Enhanced PGAT PRODUCTION training configuration
# Heavy-duty overnight training setup

# Model and training identifiers
model: Celestial_Enhanced_PGAT
model_id: celestial_enhanced_pgat_production_overnight

# Data settings
data: custom
root_path: ./data
data_path: prepared_financial_data.csv
features: MS          # Multivariate input, univariate output
target: log_Open,log_High,log_Low,log_Close     # Predict all OHLC values
embed: timeF
freq: d               # Daily frequency
use_gpu: true

# PRODUCTION Sequence settings - HEAVY DUTY
seq_len: 250          # Long sequence for better pattern recognition
label_len: 125        # Half of seq_len
pred_len: 10        # Shorter prediction horizon for better accuracy

# Data split settings - Larger sets for production
validation_length: 50   # Large validation set
test_length: 50         # Large test set

# PRODUCTION Optimization - 20 epochs overnight with advanced scheduling
learning_rate: 0.001     # Base learning rate after warmup
train_epochs: 50        # Full overnight training
batch_size: 12           # Reduced due to aggressive depth
patience: 999            # Effectively disable early stopping
lradj: warmup_cosine     # Warmup + Cosine Annealing for robust training
warmup_epochs: 5         # 5 epochs of linear warmup for faster ramp-up
min_lr: 1e-6             # Minimum learning rate at the end
weight_decay: 0.0001
clip_grad_norm: 1.0

# HEAVY MODEL hyperparameters - Maximum capacity
d_model: 416             # Match celestial feature dimension (13 Ã— 32)
n_heads: 8               # More attention heads
e_layers: 8              # Aggressive: deeper encoder
d_layers: 4              # Aggressive: deeper decoder
d_ff: 1024               # Aggressive: larger feed-forward dimension
dropout: 0.1

# Input/Output dimensions (based on prepared_financial_data.csv)
enc_in: 118    # Number of celestial wave features (CORRECTED to match actual CSV)
dec_in: 118    # Same as encoder input  
c_out: 4       # Four target outputs (OHLC)

# FULL CELESTIAL SYSTEM - MAXIMUM POWER ðŸŒŒ
use_celestial_graph: true               # Enable full celestial system
aggregate_waves_to_celestial: true      # Enable wave aggregation to celestial bodies
celestial_fusion_layers: 2              # Legacy fusion layers (ONLY used if use_petri_net_combiner=false, can cause memory issues)

# ðŸš€ PETRI NET ARCHITECTURE - DEFAULT! Zero information loss, 63Ã— memory reduction
use_petri_net_combiner: true            # âœ… ENABLED BY DEFAULT - Petri net message passing (replaces fusion layers)
num_message_passing_steps: 3            # Aggressive: more token flow iterations
edge_feature_dim: 6                     # Dimension of edge feature vectors (theta_diff, phi_diff, etc.)
use_temporal_attention: true            # Enable delayed effect modeling (attention over time)
use_spatial_attention: true             # Enable global pattern modeling (attention over nodes)

# When Petri net combiner is active, bypass the legacy spatiotemporal encoder in Step 7 to reduce overhead
bypass_spatiotemporal_with_petri: true

# Wave aggregation settings - OPTIMIZED FOR PRODUCTION
# Auto-detected celestial feature count from CSV header mapping is 113
num_input_waves: 113   # Celestial columns used by phase-aware processor
target_wave_indices: [0, 1, 2, 3]  # All OHLC targets

# Advanced features - PRODUCTION READY
use_mixture_decoder: false              # Keep deterministic for stability
use_stochastic_learner: false           # Disable for production stability
use_hierarchical_mapping: false         # Disable for production stability
use_efficient_covariate_interaction: true # Use memory-efficient partitioned graph processing
mdn_multivariate_mode: independent

# ðŸŽ¯ TARGET AUTOCORRELATION - NEW ENHANCEMENT
use_target_autocorrelation: true        # Enable target autocorrelation modeling
target_autocorr_layers: 2               # Number of LSTM layers for target processing

# ðŸ“… CALENDAR EFFECTS - NEW ENHANCEMENT
use_calendar_effects: true              # Enable calendar effects modeling
calendar_embedding_dim: 104             # Calendar embedding dimension (d_model/4)

# Regularization - PRODUCTION TUNED
reg_loss_weight: 0.0005  # Light regularization
kl_weight: 0.00005       # Very light KL regularization

# Production settings - Enhanced robustness
save_checkpoints: true
checkpoint_interval: 5   # Save every 5 epochs
log_interval: 10         # Log every 10 batches
gradient_accumulation_steps: 3  # Aggressive: offset deeper model memory; effective batch â‰ˆ 36
mixed_precision: false   # Disabled for stability (enable if GPU supports it)
save_best_only: false    # Save all checkpoints for analysis
 
# ðŸŽ² PROBABILISTIC MDN DECODER - PHASE 1 INTEGRATION
enable_mdn_decoder: false         # Enable Mixture Density Network decoder for probabilistic forecasting
mdn_components: 5                 # Number of Gaussian mixture components (K)
mdn_sigma_min: 0.001              # Minimum standard deviation floor to prevent collapse
mdn_use_softplus: true            # Use softplus for sigma (vs exp); more stable
calibration_metrics:
  coverage_levels: [0.5, 0.9]     # Nominal coverage levels for interval evaluation
  compute_crps: false             # Compute CRPS (expensive; eval-only recommended)
  crps_samples: 100               # Number of MC samples for CRPS estimation

# Diagnostics and memory logging
# Tip: leave these enabled for short debugging runs; disable for long overnight runs to minimize overhead
enable_memory_diagnostics: true   # Writes detailed snapshots to checkpoints/<run>/memory_diagnostics.log
memory_log_interval: 50           # Log every N batches (increase to reduce log volume)
collect_diagnostics: false        # Model will skip building per-batch metadata dicts to reduce transient allocations