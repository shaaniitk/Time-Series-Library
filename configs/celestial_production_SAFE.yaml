# ðŸŒŒ CELESTIAL ENHANCED PGAT - OPTIMIZED OVERNIGHT PRODUCTION CONFIG
# Updated: November 2, 2025
# With Issues #1-6, #8, #10 fixes applied for optimal convergence
# Expected runtime: 8-12 hours on single GPU

# Core Model Architecture - BALANCED DEEP
activation: gelu
d_model: 780                    # Main model dimension (13 celestial Ã— 60)
n_heads: 20                     # Attention heads (780 Ã· 20 = 39 per head)
d_ff: 3120                      # Feed-forward dimension (4 Ã— d_model)
e_layers: 8                     # Encoder layers (reduced from 12 for faster convergence)
d_layers: 4                     # Decoder layers (reduced from 6)

# FIXED: Dimensional Compatibility (Issue #1)
celestial_dim: 60               # FIXED: Compatible with n_heads (60Ã—13=780=d_model)
calendar_embedding_dim: 195     # d_model Ã· 4 = 195 (optimal size)
num_celestial_bodies: 13        # Celestial bodies count

# Data Configuration
data: custom
data_name: custom
data_path: prepared_financial_data.csv
root_path: ./data
features: MS
target: log_Open,log_High,log_Low,log_Close
target_wave_indices: [0, 1, 2, 3]
freq: d
embed: timeF

# Sequence Configuration - OPTIMIZED FOR CONVERGENCE
seq_len: 50                     # SAFE MODE: Reduced from 250 to save memory
label_len: 125                  # Decoder label length
pred_len: 10                    # Reduced from 20 for better accuracy
c_out: 4                        # Output features (OHLC)
enc_in: 118                     # Encoder input features (auto-detected: 113)
dec_in: 118                     # Decoder input features
num_input_waves: 118            # Total input waves

# Training Configuration - OPTIMIZED
batch_size: 1                   # SAFE MODE: Reduced from 4 to prevent OOM
train_epochs: 2                 # SAFE MODE: Just for testing decoder fix                # Reduced from 75 for overnight completion
learning_rate: 0.0001           # Slightly higher than 0.00008
weight_decay: 0.01              # Reduced from 0.015
gradient_accumulation_steps: 3  # Effective batch size = 12

# BALANCED: Loss Configuration for Best Convergence
loss:
  type: hybrid_mdn_directional  
  nll_weight: 0.3               # Increased from 0.15 (MDN needs signal)
  direction_weight: 5.0         # Reduced from 8.0 (was dominating)
  trend_weight: 0.8             # Increased from 0.5  
  magnitude_weight: 0.2         # Increased from 0.1
  correlation_type: pearson     

# MDN Configuration - STABLE SETUP
enable_mdn_decoder: false       # FIXED: Can't use both MDN and mixture decoder simultaneously
use_mixture_decoder: true       # FIXED: Required for hybrid_mdn_directional loss
use_sequential_mixture_decoder: false  # Single head only
mdn_components: 5               # Reduced from 8 for faster convergence
mdn_sigma_min: 0.001           
mdn_sigma_max: 1.0             
mdn_use_softplus: true         

# Advanced Features - OPTIMIZED
use_efficient_covariate_interaction: true
use_celestial_graph: true
use_celestial_target_attention: true
use_calendar_effects: true
use_phase_aware_processing: true  # FIXED Issue #5: Explicit flag
use_hierarchical_mapping: false   # Disabled for simplicity
use_multi_scale_context: true    # FIXED Issue #4: Already implemented
use_dynamic_spatiotemporal_encoder: true
use_gated_graph_combiner: false  # Disabled (using Petri net)
use_petri_net_combiner: true
use_spatial_attention: true
use_temporal_attention: true
use_target_autocorrelation: true
use_stochastic_learner: false    # Disabled for stability
use_stochastic_control: false    # Disabled for stability

# FIXED Issue #10: Stochastic Warmup
stochastic_warmup_epochs: 3     # MDN calibration period

# Celestial Configuration
aggregate_waves_to_celestial: true
celestial_fusion_layers: 4      # Reduced from 6
use_future_celestial_conditioning: false

# Graph Configuration  
num_message_passing_steps: 3    # Reduced from 4
edge_feature_dim: 24            # Reduced from 32

# Context Fusion
context_fusion_mode: multi_scale
context_fusion_layers: 2        # Reduced from 3
context_fusion_dropout: 0.1

# Petri Net Configuration
bypass_spatiotemporal_with_petri: true  # FIXED Issue #2: Soft blend implemented
petri_net_places: 39
petri_net_transitions: 52

# Stochastic Configuration (DISABLED for stability)
stochastic_temperature_start: 2.0   # Lower from 3.0
stochastic_temperature_end: 0.1    # Higher from 0.05
stochastic_decay_steps: 2000       # Faster decay

# Kernel Sizes
short_term_kernel_size: 3       # Reduced from 5
medium_term_kernel_size: 15     # Reduced from 25
long_term_kernel_size: 75       # Reduced from 125

# Regularization - BALANCED
dropout: 0.15                   # Increased from 0.1 for generalization
use_dropout_schedule: false     # Disabled for simplicity
clip_grad_norm: 1.0             # Increased from 0.8
reg_loss_weight: 0.001          # Reduced from 0.002
label_smoothing: 0.0            # Disabled

# Learning Rate Schedule
lradj: warmup_cosine
warmup_epochs: 8
min_lr: 0.000000005
cosine_restarts: 2

# Optimization
use_lookahead_optimizer: false
lookahead_k: 5
lookahead_alpha: 0.5
ema_decay: 0.9999

# Normalization
use_layer_norm: true
use_batch_norm: false
use_spectral_norm: true
scale: true

# Advanced Training Features
mixed_precision: false
use_amp: false
deterministic: true
use_curriculum_learning: false
curriculum_start_seq_len: 100
curriculum_end_seq_len: 500
curriculum_steps: 10

# Ensemble Configuration
use_ensemble_prediction: false
ensemble_size: 3
ensemble_method: weighted_average

# Progressive Growing
use_progressive_growing: false

# Knowledge Distillation
use_knowledge_distillation: false

# Multi-GPU
use_multi_gpu: false

# Calibration Metrics
calibration_metrics:
  compute_crps: true
  compute_energy_score: true
  compute_variogram_score: true
  crps_samples: 300
  coverage_levels: [0.5, 0.68, 0.8, 0.9, 0.95, 0.99]

# Adversarial Testing
enable_adversarial_checks: true
adversarial_noise_std: 0.08
adversarial_scale_factor: 0.25
adversarial_dropout_ratio: 0.12
adversarial_max_batches: 12

# System Configuration
use_gpu: false              # FORCED CPU MODE for debugging
gpu: 0
devices: '0'
num_workers: 0
pin_memory: false           # Disabled for CPU mode
persistent_workers: false
prefetch_factor: 2

# Memory Management
max_memory_usage: 0.85
enable_memory_diagnostics: false
enable_memory_snapshots: true
memory_log_interval: 100
memory_snapshot_interval: 100
memory_summary_on_exception: true
dump_cuda_memory_summary: false

# Checkpointing
checkpoints: ./checkpoints/
save_checkpoints: true
save_best_only: false
checkpoint_interval: 3

# Early Stopping
patience: 25
early_stopping_patience: 30
early_stopping_min_delta: 0.000001
early_stopping_restore_best_weights: true

# Logging
log_level: INFO
log_format: '%(asctime)s | %(levelname)-8s | %(name)s | %(message)s'
log_file: logs/production_ultimate_deep_training_fixed.log
log_interval: 10                # Show batch progress every 10 batches
verbose_logging: false
debug_mode: true
debug_log_file: logs/debug_training_fixed.log

# Diagnostics
collect_diagnostics: false
log_gradient_norms: false
log_weight_norms: false

# Tracking
track_training_time: true
track_memory_usage: true
track_gpu_utilization: true
track_learning_rate: true
track_loss_components: true

# Seeds
seed: 42
random_seed: 42
production_seed: 42

# Task Configuration
task_name: ultra_long_term_forecast
model: Celestial_Enhanced_PGAT
model_name: Celestial_Enhanced_PGAT
model_id: celestial_ultimate_deep_500x20_fixed

# Legacy Compatibility
inverse: false
cols: null
itr: 1
train_only: false
do_predict: false
output_attention: false
factor: 1
distil: true
mix: true
seasonal_patterns: Monthly

# Validation/Test Configuration
validation_length: 100
test_length: 100